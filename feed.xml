<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en_UK"><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://aeporreca.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://aeporreca.org/" rel="alternate" type="text/html" hreflang="en_UK" /><updated>2020-07-11T08:09:46+02:00</updated><id>https://aeporreca.org/feed.xml</id><title type="html">Antonio E. Porreca</title><subtitle>Antonio E. Porreca’s Website</subtitle><author><name>Antonio E. Porreca</name><email>antonio.porreca@lis-lab.fr</email></author><entry><title type="html">The semiring of dynamical systems</title><link href="https://aeporreca.org/blog/semiring-of-dynamical-systems/" rel="alternate" type="text/html" title="The semiring of dynamical systems" /><published>2020-06-28T00:00:00+02:00</published><updated>2020-06-28T00:00:00+02:00</updated><id>https://aeporreca.org/blog/semiring-of-dynamical-systems</id><content type="html" xml:base="https://aeporreca.org/blog/semiring-of-dynamical-systems/">This is a post about something I’m working on at the moment, the semiring of finite, discrete time dynamical systems. Here’s a picture to get you interested, a portion of the multiplication table of dynamical systems (everyone loves this️).

&lt;img src=&quot;../files/product-table-of-dynamical-systems.svg&quot; style=&quot;max-width: 100%&quot;&gt;

Here, a dynamical system $(A,f)$ is just a finite set $A$ of states (including $\varnothing$) with a state transition function $f \colon A \to A$ (any function will do). Each such dynamical system consists of one or more limit cycles with oriented trees having the points of those cycles as their roots.

You can define operations on dynamical systems (modulo isomorphism) and obtain a commutative semiring $(\mathbf{D},+,×)$.

The sum $+$ is defined as $(A,f) + (B,g) = (A⊎B, f+g)$ where $(f+g)(x) = f(x)$ if $x ∈ A$ and $(f+g)(x) = g(x)$ if $x ∈ B$. The empty dynamical system $0_{\mathbf{D}} = (\varnothing, \varnothing)$ is the identity of $+$.

The product $×$ is $(A,f) × (B,g) = (A×B, f×g)$ where $(f×g)(x,y) = (f(x),g(y))$. The dynamical system with one point $1_{\mathbf{D}} = (\\{x\\},\mathrm{id})$ is the identity of $×$.

In terms of graphs of the dynamics $(V,E) = (A, \\{ (x,f(x)) : x ∈ A \\})$, the sum is disjoint union and the product is the tensor (or Kronecker) product.

$(\mathbf{D},+,×)$ has the natural numbers $(ℕ,+,×)$ as a subsemiring. It’s the image of the injective homomorphism $ϕ: ℕ → \mathbf{D}$ defined as $ϕ(n) =$ the dynamical system consisting of exactly n fixed points. These behave as expected with respect to the others, e.g., $ϕ(n) × A = \underbrace{A + ⋯ + A}_{n \text{ times}}$.

$(\mathbf{D},+,×)$ is quite complex as a semiring. There are infinitely many elements with multiple factorisations into irreducibles, the smallest one being $C₂+C₂ = (C₂)² = 2C₂$, where $C₂$ is the cycle of length two (see the table above). We don’t know yet which are its prime elements, or if any exists at all.

Furthermore, the majority of elements of $\mathbf{D}$ are irreducible. More precisely, we have $\lim_{n \to \infty} \frac{\text{\# reducible dyn. systems of size } n}{\text{\# dyn. systems of size } n} = 0$. This was proved by [Valentina Dorigatti](https://www.uninsubria.it/hpp/valentina.dorigatti) in her master’s thesis.

You can study a form of (de)composition of dynamical systems by considering polynomial equations. In the following the polynomials are always over $\mathbf{D}[\vec{X}]$ or its subsemiring $ℕ[\vec{X}]$, with many variables $\vec{X} = (X_1,…,X_n)$. The general form of an equation is $p(\vec{X}) = q(\vec{X})$, since you cannot move everything to the left-hand side due to the lack of subtraction.

Each equation $p(\vec{X}) = q(\vec{X})$ over $\mathbf{D}[\vec{X}]$ has an associated equation $p(\vec{X}) = q(\vec{X})$ over $ℕ[\vec{X}]$, obtained by replacing each dynamical system $A$ by its size $\|A\|$. If $p,q ∈ ℕ[\vec{X}]$ are polynomials over $ℕ$ and $(A_1,…A_n)$ is a solution in $\mathbf{D}$ to $p(\vec{X}) = q(\vec{X})$, then $(\|A_1\|,\|A_2\|,…)$ is another solution, this time over the naturals.

So, by searching for solutions in the larger semiring $\mathbf{D}$ we might sometimes find new solutions to equations with natural coefficients, but only if a natural solution already exists. This means that Hilbert’s 10th problem over $\mathbf{D}$ has a negative answer! There is no algorithm for deciding if an equation over $\mathbf{D}[\vec{X}]$ has a solution (much less finding one), otherwise you could use that algorithm for $ℕ[\vec{X}]$ too.

Equations with a constant side like $p(\vec{X}) = A$ with $p ∈ \mathbf{D}[\vec{X}]$ and $A ∈ \mathbf{D}$ are decidable and in $\mathsf{NP}$: since $+$ and $×$ are monotonic with respect to the size of the dynamical systems, the value $\|A\|$ gives you a bound to the size of the solution, which you can guess and check in polynomial time.

You can do the check in polynomial time because the graphs of the dynamics, being just some cycles with trees going into them, are planar; the graph isomorphism problem for planar graphs is in $\mathsf{L} ⊆ \mathsf{P}$, and actually [complete for it](https://doi.org/10.1109/CCC.2009.16).

Unfortunately, deciding if $p(\vec{X}) = A$ has a solution is $\mathsf{NP}$-complete. We can prove it by reduction from One-in-three 3SAT, the problem of finding if a Boolean formula $\varphi$ in ternary conjunctive normal form has a satisfying assignment where exactly one literal per clause is true.

For each variable $X$ of formula $\varphi$ we construct an equation $X + X' = 1$. Here $X'$ represents the literal $¬X$, and exactly one of $X$ and $X'$ will be $1$, while the other will be $0$. For each clause, e.g., $(X_1 ∨ ¬X_2 ∨ X_3)$, we have an equation $X_1 + X_2' + X_3 = 1$, which forces exactly one literal to $1$, and the others to $0$.

This gives us a system of $m$ linear equations of the form $p_i(\vec{X}) = 1$. By multiplying all left- and right-hand sides we get $∏_{i=1}^m p_i(\vec{X}) = 1$, which has the same solution as the system of linear equations. This proves the $\mathsf{NP}$-hardness, but only uses the subsemiring $ℕ$ of $\mathbf{D}$.

With a little more work, and thanks to the colleague [Florian Bridoux](https://pageperso.lis-lab.fr/florian.bridoux/) and the research intern Émile Touileb I’m currently supervising, we can prove that deciding if a *single* linear equation over $\mathbf{D}[\vec{X}]$ has solution is $\mathsf{NP}$-complete; this finally exploits its non-natural elements.

With another research intern I supervised, Caroline Gaze-Maillot, we have recently analysed dynamical systems $(A,f)$ more abstractly, in terms of their “profiles” $(\|A\|_1,\|A\|_2,…)$, where $\|A\|_i$ is the number of points at distance $i$ (in terms of number of applications of $f$) from their limit cycle. The term is inspired by the topographic profile of a terrain, and here’s an example of dynamical system having profile $(8, 7, 8, 4)$:

&lt;img src=&quot;../files/profile-of-dynamical-system.svg&quot; style=&quot;max-width: 100%&quot;&gt;

(In case you’re interested, the contour lines in this picture are an approximation, in reverse order of altitude, of those of [Mont Puget](https://en.wikipedia.org/wiki/Mont_Puget), a nice mountain which is just a 45 minute walk away from my office in Luminy, Marseille.)

We discovered that the profiles of dynamical systems also form a commutative semiring $(\mathbf{P},+,×)$, with “taking the profile” being a semiring homomorphism $\mathbf{D} → \mathbf{P}$, and this is also complicated stuff, with multiple factorisations, polynomial equations that are undecidable or $\mathsf{NP}$-complete, etc.

Shout-out to everyone who worked on this: [Florian Bridoux](https://pageperso.lis-lab.fr/florian.bridoux/), [Alberto Dennunzio](https://www.unimib.it/alberto-dennunzio), [Valentina Dorigatti](https://www.uninsubria.it/hpp/valentina.dorigatti), [Enrico Formenti](http://mc3.i3s.unice.fr/~formenti/), Caroline Gaze-Maillot, [Maximilien Gadouleau](https://community.dur.ac.uk/m.r.gadouleau/), [Luca Manzoni](https://dssc.units.it/people/luca-manzoni), [Luciano Margara](http://www.cs.unibo.it/~margara/), [Valentin Montmirail](http://www.valentin-montmirail.com), [yours truly](https://aeporreca.org), [Sara Riva](http://www.i3s.unice.fr/~riva/), and Émile Touileb.

Bibliography
------------

1. Dennunzio, A., Dorigatti, V., Formenti, E., Manzoni, L., Porreca, A.E.: Polynomial equations over finite, discrete-time dynamical systems. In: Mauri, G., El Yacoubi, S., Dennunzio, A., Nishinari, K., and Manzoni, L. (eds.) *Cellular Automata, 13th International Conference on Cellular Automata for Research and Industry, ACRI 2018*. pp. 298–306. Springer (2018). [[link](https://doi.org/10.1007/978-3-319-99813-8_27)] [[preprint](/papers/polynomial-equations-over-finite-discrete-time-dynamical-systems.pdf)]

2. Dennunzio, A., Formenti, E., Margara, L., Montmirail, V., Riva, S.: Solving equations on discrete dynamical systems (extended version) (2019). [[preprint](https://arxiv.org/abs/1904.13115)]

3. Porreca, A.E.: Composing behaviours in the semiring of dynamical systems, invited talk at *International Workshop on Boolean Networks (IWBN 2020)*, Universidad de Concepción, Chile (2020). (Talk given by Florian Bridoux due to my inability to attend the workshop.) [[slides](/talks/composing-behaviours-in-semiring-of-dynamical-systems.pdf)]

&lt;!--  LocalWords:  semiring homomorphism
 --&gt;</content><author><name>Antonio E. Porreca</name><email>antonio.porreca@lis-lab.fr</email></author><summary type="html">This is a post about something I’m working on at the moment, the semiring of finite, discrete time dynamical systems. Here’s a picture to get you interested, a portion of the multiplication table of dynamical systems (everyone loves this️).</summary></entry><entry><title type="html">Nash beats Gödel: On the history of complexity and cryptography</title><link href="https://aeporreca.org/blog/nash-beats-godel/" rel="alternate" type="text/html" title="Nash beats Gödel: On the history of complexity and cryptography" /><published>2012-02-17T00:00:00+01:00</published><updated>2012-02-17T00:00:00+01:00</updated><id>https://aeporreca.org/blog/nash-beats-godel</id><content type="html" xml:base="https://aeporreca.org/blog/nash-beats-godel/">We know that Kurt Gödel, unhappy with having only completeness and incompleteness theorems named after him, also essentially invented the &lt;strong&gt;P&lt;/strong&gt; vs &lt;strong&gt;NP&lt;/strong&gt; question in a &lt;a href=&quot;http://rjlipton.wordpress.com/the-gdel-letter/&quot;&gt;1956 letter to John Von Neumann&lt;/a&gt;. Unfortunately, there were no blogs back then, so we had to wait until the 1980s to read it, and Cook, Karp &amp; co. had to reinvent the question from scratch.

What we &lt;em&gt;didn't&lt;/em&gt; know until a few days ago (I personally discovered it today via &lt;a href=&quot;http://aaronsadventures.blogspot.com/2012/02/amazing-new-declassified-document.html&quot;&gt;Aaron Roth&lt;/a&gt;) is that in 1955, one year before Gödel's letter, &lt;em&gt;John Nash&lt;/em&gt; also had something to say on the topic, in a few letters to NSA. Gödel and Nash independently made &lt;em&gt;strikingly&lt;/em&gt; similar remarks, starting from completely different points of view. While Gödel's perspective was that of a logician (his letter was about the length of proofs in first-order logic), Nash was interested in cryptography, and wrote about the security of cryptographic systems.

&lt;a href=&quot;http://www.nsa.gov/public_info/press_room/2012/nash_exhibit.shtml&quot;&gt;NSA's press release&lt;/a&gt; about the declassification of Nash's letters was apparently made on 27 January 2012; &lt;a href=&quot;http://www.nsa.gov/public_info/_files/nash_letters/nash_letters1.pdf&quot;&gt;here&lt;/a&gt; is a PDF containing Nash's letters and the replies he got from NSA. I'll only focus on the second letter here, leaving a cryptoanalysis of the system he proposed in the third one to someone more knowledgeable than me on the topic.

I assume Nash was familiar with Shannon's work &lt;em&gt;Communication theory of secrecy systems&lt;/em&gt; (1949), so he already knew that the only mathematically unbreakable cipher is one-time pad. Hence, in order to discuss practical encryption systems, he invented the principles of modern cryptography about 20 years in advance:

&lt;blockquote style=&quot;border:none;&quot;&gt;If &lt;span style=&quot;font-style:normal;&quot;&gt;[&lt;/span&gt;the computation required in order to recover the key from the ciphertext&lt;span style=&quot;font-style:normal;&quot;&gt;]&lt;/span&gt;, although possible in principle, were sufficiently long at best then the process could still be secure in a practical sense.&lt;/blockquote&gt;

Here is some computational complexity, 8 years before Hartmanis and Stearns:

&lt;blockquote style=&quot;border:none;&quot;&gt;The most direct computation procedure would be for the enemy to try all &lt;span style=&quot;font-style:normal;&quot;&gt;2&lt;/span&gt;&lt;sup&gt;&lt;em&gt;r&lt;/em&gt;&lt;/sup&gt; possible keys, one by one. Obviously this is &lt;span style=&quot;font-style:normal;&quot;&gt;easily&lt;/span&gt; made impractical for the enemy by simply choosing &lt;em&gt;r&lt;/em&gt; large enough.&lt;/blockquote&gt;

Apparently Nash already knew quite well (&quot;obviously&quot;, &quot;easily&quot;) that exponential time is too much.

And here is how one can classify cryptosystems (but also general computation problems) according to him. I think you can read essentially the same paragraph in all modern complexity theory and cryptography books.

&lt;blockquote style=&quot;border:none;&quot;&gt;So a logical way to classify enciphering processes is by the way in which the computation length for the computation of the key increases with increasing length of the key. This is at best exponential and at worst probably a relatively small power of &lt;em&gt;r&lt;/em&gt;, &lt;em&gt;ar&lt;/em&gt;&lt;sup&gt;&lt;span style=&quot;font-style:normal;&quot;&gt;2&lt;/span&gt;&lt;/sup&gt; or &lt;em&gt;ar&lt;/em&gt;&lt;sup&gt;&lt;span style=&quot;font-style:normal;&quot;&gt;3&lt;/span&gt;&lt;/sup&gt;, as in substitution ciphers.&lt;/blockquote&gt;

Nash also conjectures that many problems cannot be solved in polynomial time. In hindsight, there is some naivety here about the design of good ciphers, but this is still very interesting:

&lt;blockquote style=&quot;border:none;&quot;&gt;Now my general conjecture is as follows: For almost all sufficiently complex types of enciphering, especially where the instructions given by different portions of the key interact complexly with each other in the determination of their ultimate effects on the enciphering, the mean key computation length increases exponentially with the length of the key, or in other words, with the information content of the key.

The significance of this general conjecture, assuming its truth, is easy to see. It means that it is quite feasible to design ciphers that are effectively unbreakable. As ciphers became more sophisticated the game of cipher breaking by skilled teams, etc., should become a thing of the past.&lt;/blockquote&gt;

All in all, this seems to me a truly magnificent historical document, possibly on par with the aforementioned letter by Gödel. I'd really like to know what people working in cryptography think about this.

These &quot;Lost Letter&quot; discoveries also make me wonder what further treasures may be hidden in NSA's cardboard boxes, or in private correspondence collections. What if there exists a “Fermat's Lost (Last?) Letter” along the lines of &quot;Here's the proof that didn't fit in the margin&quot;, or a “Russell's Lost Letter” that he sent to Whitehead to tell him “I found a proposition that can't be proved nor refuted in &lt;em&gt;Principia Mathematica&lt;/em&gt;”?</content><author><name>Antonio E. Porreca</name><email>antonio.porreca@lis-lab.fr</email></author><summary type="html">We know that Kurt Gödel, unhappy with having only completeness and incompleteness theorems named after him, also essentially invented the P vs NP question in a 1956 letter to John Von Neumann. Unfortunately, there were no blogs back then, so we had to wait until the 1980s to read it, and Cook, Karp &amp;amp; co. had to reinvent the question from scratch.</summary></entry><entry><title type="html">Do waterfalls play chess? and other stories</title><link href="https://aeporreca.org/blog/do-waterfalls-play-chess/" rel="alternate" type="text/html" title="Do waterfalls play chess? and other stories" /><published>2011-08-13T00:00:00+02:00</published><updated>2011-08-13T00:00:00+02:00</updated><id>https://aeporreca.org/blog/do-waterfalls-play-chess</id><content type="html" xml:base="https://aeporreca.org/blog/do-waterfalls-play-chess/">I finally took the time to read &lt;a href=&quot;http://www.scottaaronson.com/&quot;&gt;Scott Aaronson&lt;/a&gt;’s new paper &lt;a href=&quot;http://eccc.hpi-web.de/report/2011/108/&quot;&gt;Why philosophers should care about computational complexity&lt;/a&gt; (there's a &lt;a href=&quot;http://www.scottaaronson.com/blog/?p=735&quot;&gt;post&lt;/a&gt; about it with lots of comments on his &lt;a href=&quot;http://www.scottaaronson.com/blog/&quot;&gt;blog&lt;/a&gt;).

Summary: Aaronson, knowing that I’m writing my thesis at the moment, took the liberty of preparing an introduction for me. Thanks! I'm going to include it verbatim. (Just kidding of course, though I'm actually going to cite this paper extensively.)

&lt;a href=&quot;http://www.flickr.com/photos/phalaenopsisaphrodite523/5148870343/&quot; title=&quot;Red Queen by Phalaenopsis Aphrodite, on Flickr&quot;&gt;&lt;img src=&quot;http://farm5.static.flickr.com/4008/5148870343_08cc5cf697.jpg&quot; alt=&quot;Red Queen&quot; class=&quot;aligncenter size-full wp-image-863&quot;&gt;&lt;/a&gt;

The early work on computability had a lot of philosophical significance; actually, we can say that computability was &lt;em&gt;born&lt;/em&gt; to answer a philosophical question, Hilbert's &lt;a href=&quot;http://en.wikipedia.org/wiki/Entscheidungsproblem&quot;&gt;&lt;em&gt;Entscheidungsproblem&lt;/em&gt;&lt;/a&gt;. We all know that the answer turned out to be negative (or do we? See Section 3.1). Apparently computational complexity theory, the theory of computing under limited resources, didn't turn out to be so popular among philosophers. That's &lt;a href=&quot;http://aeporreca.org/2010/12/02/philosophical-quest/&quot;&gt;obviously&lt;/a&gt; a problem, and this paper tries to do something about it.

After a brief introduction to complexity theory (Section 2), Aaronson turns his attention to one of the main cornerstones of this field, which is also one the points that are usually criticised: the relevance of polynomial time, as opposed to exponential time. Here he argues that this distinction is at least as interesting as the distinction between computable and uncomputable. Section 3.3 contains an interesting question that can be answered using a complexity-theoretic argument: why would we call &lt;a href=&quot;http://en.wikipedia.org/wiki/Mersenne_prime&quot;&gt;2&lt;sup&gt;43112609&lt;/sup&gt; &amp;minus; 1&lt;/a&gt; (together with a proof of its primality) a “known” prime, while “the first prime larger than 2&lt;sup&gt;43112609&lt;/sup&gt; &amp;minus; 1” feels somehow “unknown”?

Section 4 is about the &lt;a href=&quot;http://en.wikipedia.org/wiki/Turing_test&quot;&gt;Turing test&lt;/a&gt;. This is the first time I see in print what Aaronson calls the “lookup-table argument” (though he cites &lt;a href=&quot;http://philpapers.org/rec/BLOSAA&quot;&gt;Ned Block&lt;/a&gt; and others for it): namely, that there’s no doubt whatsoever that an abstract program &lt;em&gt;P&lt;/em&gt; able to pass the Turing test does actually exist (you just need a huge but finite table of replies dependent on the previous history of the conversation). The only way to claim the impossibility of a machine passing the Turing test, apart from metaphysical arguments, requires addressing concerns such as the efficiency of &lt;em&gt;P&lt;/em&gt; or the availability of enough space in the universe to store it. That is, complexity-theoretic questions.

Section 5, possibly the most interesting one, addresses the problem of logical omniscience. It is usually assumed that knowledge is closed under deduction rules; for instance, if I know &lt;em&gt;A&lt;/em&gt;, and I know &lt;em&gt;B&lt;/em&gt;, then I certainly know &lt;em&gt;A&lt;/em&gt; &amp;and; &lt;em&gt;B&lt;/em&gt;. But consider this: while I know enough basic axioms of maths, I surely don’t &lt;em&gt;know&lt;/em&gt; that Fermat’s last theorem holds (except by trusting the mathematical community, which I usually do), even though it’s a mere deductive consequence of said axioms. We can argue that human beings do not possess the computational resources needed to actually achieve omniscience.

Section 6 is about a “waterfall argument” against &lt;a href=&quot;http://en.wikipedia.org/wiki/Computational_theory_of_mind&quot;&gt;computationalism&lt;/a&gt;. The idea is that the meaning of what a supposed computer actually computes is always imposed by someone looking at it, that is, it’s always &lt;em&gt;relative to some external observer&lt;/em&gt;. I freely admit I am (was?) a fan of this argument, though I don’t necessarily see it as an attack to computationalism (I use to call this “computational relativism”, a term whose invention I claim, since Google returns zero hits for it :-)). According to some proponents, this leads to some “strange” consequences.

&lt;a href=&quot;http://www.flickr.com/photos/crabbylioncardsandmore/4980927493/&quot; title=&quot;Niagara Falls by Kevin Timothy, on Flickr&quot;&gt;&lt;img src=&quot;http://farm5.static.flickr.com/4084/4980927493_38dc298025.jpg&quot; width=&quot;500&quot; height=&quot;335&quot; alt=&quot;Niagara Falls&quot; class=&quot;aligncenter size-full wp-image-863&quot;&gt;&lt;/a&gt;

For instance, assume that we encode chess positions in the physical states of a waterfall, then take a look at some “final” state of the waterfall, and once again interpret that as a chess position. Can the waterfall be said to play chess? Aaronson argues that it is not so, unless the encoding (which is just a &lt;a href=&quot;http://en.wikipedia.org/wiki/Reduction_(complexity)&quot;&gt;reduction&lt;/a&gt; from chess to computing the state of the waterfall) can be computed by a procedure requiring &lt;em&gt;less&lt;/em&gt; resources than those needed to actually compute the state of the waterfall. But then a question emerges: does &lt;a href=&quot;http://dx.doi.org/10.1126/science.7973651&quot;&gt;DNA&lt;/a&gt; compute Hamilton paths?

Section 7 describes how computational complexity can help to define what inductive inference is about, including Occam’s razor and the &lt;a href=&quot;http://en.wikipedia.org/wiki/Grue_and_bleen&quot;&gt;“new riddle of induction”&lt;/a&gt;.

Then, in Section 8, Aaronson argues that complexity theory might inspire some debate about the interpretation of quantum physics, particularly about the &lt;a href=&quot;http://en.wikipedia.org/wiki/Many-worlds_interpretation&quot;&gt;many-worlds interpretation&lt;/a&gt;.

In Section 9 new notions of proof, quite distinct from the usual notion of “formal” and “informal” proofs, are described. These are all based on complexity-theoretic and cryptographic results: &lt;a href=&quot;http://en.wikipedia.org/wiki/Interactive_proof_system&quot;&gt;interactive proof systems&lt;/a&gt;, &lt;a href=&quot;http://en.wikipedia.org/wiki/Zero-knowledge_proof&quot;&gt;zero-knowledge proofs&lt;/a&gt;, &lt;a href=&quot;http://en.wikipedia.org/wiki/Probabilistically_checkable_proof&quot;&gt;probabilistically checkable proofs&lt;/a&gt;.

Section 10 is about the difference between time and space. This is usually stated as “space can be reused, time cannot”. What if we allow time travel (i.e., &lt;a href=&quot;http://en.wikipedia.org/wiki/Closed_timelike_curve&quot;&gt;closed timelike curves&lt;/a&gt;)? The assumption that &lt;strong&gt;P&lt;/strong&gt; &amp;ne; &lt;strong&gt;NP&lt;/strong&gt; might suggest an argument for the impossibility of time travel, even assuming the validity of quantum physics.

Section 11 is about economics and game theory; here complexity theory can be useful to describe the behaviour of agents under &lt;a href=&quot;http://en.wikipedia.org/wiki/Bounded_rationality&quot;&gt;bounded rationality&lt;/a&gt; assumptions.

Finally, in Section 12, Aaronson concludes that &lt;em&gt;lots&lt;/em&gt; of philosophical problems seem to have interesting complexity-theoretic aspects. Several questions remain open, and the most important of those is still “If &lt;strong&gt;P&lt;/strong&gt; &amp;ne; &lt;strong&gt;NP&lt;/strong&gt;, then how have humans managed to make such enormous mathematical progress, even in the face of the general intractability of theorem-proving?”.

I really hope that Aaronson’s paper spurs a lot of philosophical discussion involving and concerning complexity theory; I too believe there’s much to write about that.</content><author><name>Antonio E. Porreca</name><email>antonio.porreca@lis-lab.fr</email></author><summary type="html">I finally took the time to read Scott Aaronson’s new paper Why philosophers should care about computational complexity (there’s a post about it with lots of comments on his blog).</summary></entry><entry><title type="html">Time is unpredictable</title><link href="https://aeporreca.org/blog/time-is-unpredictable/" rel="alternate" type="text/html" title="Time is unpredictable" /><published>2011-02-19T00:00:00+01:00</published><updated>2011-02-19T00:00:00+01:00</updated><id>https://aeporreca.org/blog/time-is-unpredictable</id><content type="html" xml:base="https://aeporreca.org/blog/time-is-unpredictable/">*This post is inspired by the question [Are runtime bounds in P decidable?](http://cstheory.stackexchange.com/q/5004/182) asked by [John Sidles](http://www.mrfm.org/) on the CSTheory website, and the [answer](http://cstheory.stackexchange.com/questions/5004/are-runtime-bounds-in-p-decidable-answer-no/5006#5006) given by [Emanuele Viola](http://www.ccs.neu.edu/home/viola/).*

Is there an automatic procedure to determine whether a given Turing machine, *known* to be halting, operates within time bound $O(f)$ (assuming $f$ is a computable function)?

Predictably, the answer turns out to be negative.

Let’s start by formalising the problem. Assume $M$ is the Turing machine whose runtime we’re interested in, and $F$ is another TM computing the time bound $f$; then

$$L = \{ (M, F) : M \text{ halts within } O(F) \text{ steps} \}.$$

Also let $H$ be the halting problem:

$$H = \{ (M’, x) : M \text{ halts on input } x \}.$$

We can now define the function $R(M’, x) = (M, F)$, where $F$ computes $n^2$ and $M$, on input $y$, behaves as follows:

- Simulate $M'$ on $x$ for $n = \lvert y \rvert$ steps.
- If $M'$ has already halted, then loop for $n^2$ steps.
- Otherwise, loop for $n^3$ steps.

We’re finally ready to prove our undecidability result.

**Theorem.** $R$ is a many-one reduction from $H$ to $L$.

*Proof.* Clearly $R$ is a computable function, so we just need to show that

$$(M', x) \in H \iff (M, F) \in L.$$

If  $(M', x) \in H$, that is, if $M'$ halts on input $x$, then it does so in $k$ steps (for some $k \in \mathbb{N}$). Hence $M$ runs in $O(n^2)$ time (notice that it runs in $n^3$ time for $\lvert y \rvert &lt; k$, but it's only the asymptotic behaviour that matters for us). Thus $(M, F) \in L$.

On the other hand, if $M'$ does not halt on $x$, then $M$ never completes its simulation, and the runtime for $M$ is $O(n^3)$. Thus $(M, F) \notin L$. □

**Remark.** We’ve actually proved a stronger statement, i.e., that the set of Turing machines running in $O(n^2)$ time is undecidable, even if we restrict the domain to machines halting in polynomial time. Similar undecidability results hold for an infinite class of time bounds.</content><author><name>Antonio E. Porreca</name><email>antonio.porreca@lis-lab.fr</email></author><summary type="html">Is there an automatic procedure to determine whether a given Turing machine, *known* to be halting, operates within time bound $O(f)$ (assuming $f$ is a computable function)?</summary></entry><entry><title type="html">Merry Christmath!</title><link href="https://aeporreca.org/blog/merry-christmath/" rel="alternate" type="text/html" title="Merry Christmath!" /><published>2011-01-04T00:00:00+01:00</published><updated>2011-01-04T00:00:00+01:00</updated><id>https://aeporreca.org/blog/merry-christmath</id><content type="html" xml:base="https://aeporreca.org/blog/merry-christmath/">&gt; On the second day of Christmas my true love gave to me… The only even prime and the absolute value of &lt;em&gt;e&lt;/em&gt; to the &lt;em&gt;iπ&lt;/em&gt;.

A short post, just because I want this on my blog: a beautiful [video](http://vihart.com/blog/gauss/) and [song](http://vihart.com/music/gauss12days.mp3) by [Vi Hart](http://vihart.com) (take a look at the rest of her website, it’s amazing). Epsilon greater-than to you too.

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/sxnX5_LbBDU&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;

Also, thanks to &lt;a href=&quot;http://divisbyzero.com/&quot;&gt;Dave Richeson&lt;/a&gt; for &lt;a href=&quot;http://twitter.com/divbyzero/status/22311164692140032&quot;&gt;sharing&lt;/a&gt; it in the first place.</content><author><name>Antonio E. Porreca</name><email>antonio.porreca@lis-lab.fr</email></author><summary type="html">A short post, just because I want this on my blog: a beautiful [video](http://vihart.com/blog/gauss/) and [song](http://vihart.com/music/gauss12days.mp3) by [Vi Hart](http://vihart.com) (take a look at the rest of her website, it’s amazing). Epsilon greater-than to you too.</summary></entry><entry><title type="html">Undecidability in terms of complexity</title><link href="https://aeporreca.org/blog/undecidability-as-complexity/" rel="alternate" type="text/html" title="Undecidability in terms of complexity" /><published>2010-12-13T00:00:00+01:00</published><updated>2010-12-13T00:00:00+01:00</updated><id>https://aeporreca.org/blog/undecidability-as-complexity</id><content type="html" xml:base="https://aeporreca.org/blog/undecidability-as-complexity/">In his classic book &lt;a href=&quot;http://books.google.com/books?id=JogZAQAAIAAJ&quot;&gt;&lt;em&gt;Computational Complexity&lt;/em&gt;&lt;/a&gt;, Papadimitriou writes (page 59) that

&gt; Undecidability is in some sense the most lethal form of complexity.

I’ve &lt;a href=&quot;http://mathoverflow.net/questions/8632/cut-elimination/8634#8634&quot;&gt;just come across&lt;/a&gt; a paper with a very interesting remark along the same lines: &lt;a href=&quot;http://www.ams.org/journals/bull/1997-34-02/S0273-0979-97-00715-5/&quot;&gt;Making proofs without Modus Ponens: An introduction to the combinatorics and complexity of cut elimination&lt;/a&gt; by A. Carbone and S. Semmes, published in the &lt;em&gt;Bulletin of the American Mathematical Society&lt;/em&gt;. This paper is about the length of proofs (a subject I’ve already touched upon &lt;a href=&quot;http://aeporreca.org/2010/07/04/length-of-proofs/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://aeporreca.org/2010/07/12/length-of-proofs-2/&quot;&gt;here&lt;/a&gt;) without &lt;a href=&quot;http://en.wikipedia.org/wiki/Cut-elimination_theorem&quot;&gt;cut elimination&lt;/a&gt;.

On page 113, the authors write

&gt; By contrast [with propositional logic] in predicate logic one typically faces issues of algorithmic decidability or undecidability. That is, whether there is an algorithm at all that always gives the right answer, never mind how long it takes. The problem of determining whether a formula in predicate logic is a tautology is algorithmically undecidable. One can think of this as a matter of complexity, as follows. The tautologies in predicate logic of length at most $n$ form a finite set for which one can choose a finite set of proofs. Let $f(n)$ denote the maximum of the lengths of the shortest proofs of tautologies of length $\le n$. The fact that there is no algorithm for determining whether or not a formula is a tautology means that $f(n)$ grows very fast, faster than any recursive function.

Essentially, this means that we can’t design an algorithm to solve an undecidable problem because the search space is too large, and the only way to somehow bound it is via a function that grows faster than any computable function. Compare this with the search space of the “witnesses” for an &lt;strong&gt;NP&lt;/strong&gt;-complete problem: its size is exponential, which is too large to be explored efficiently (at least, as far as we know at the moment), but still small enough to make the problem decidable.</content><author><name>Antonio E. Porreca</name><email>antonio.porreca@lis-lab.fr</email></author><summary type="html">In his classic book [*Computational Complexity*](http://books.google.com/books?id=JogZAQAAIAAJ), Papadimitriou writes (page 59) that *Undecidability is in some sense the most lethal form of complexity*.</summary></entry><entry><title type="html">Position paper: Computing with LEGO</title><link href="https://aeporreca.org/blog/position-paper-computing-with-lego/" rel="alternate" type="text/html" title="Position paper: Computing with LEGO" /><published>2010-12-10T00:00:00+01:00</published><updated>2010-12-10T00:00:00+01:00</updated><id>https://aeporreca.org/blog/position-paper-computing-with-lego</id><content type="html" xml:base="https://aeporreca.org/blog/position-paper-computing-with-lego/">There is a LEGO Turing machine, constructed at Aarhus University (see &lt;a href=&quot;http://legoofdoom.blogspot.com&quot;&gt;here&lt;/a&gt; for more information):

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/cYw2ewoO6c4&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;

There is also a &lt;a href=&quot;http://acarol.woz.org/difference_engine.html&quot;&gt;LEGO Babbage difference engine&lt;/a&gt; built by Andrew Carol:

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/i_u3hpYMySk&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;

Now we also have an &lt;a href=&quot;http://acarol.woz.org/antikythera_mechanism.html&quot;&gt;Antikythera mechanism&lt;/a&gt; (an ancient Greek computer for predicting eclipses) made of LEGO, again by Andrew Carol:

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/RLPVCJjTNgk&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;

(Thanks to &lt;a href=&quot;http://twitter.com/CompSciFact&quot;&gt;CompSciFact&lt;/a&gt; on Twitter for the last one.)

I hereby propose that LEGO brick constructions are to be considered serious unconventional computing devices, and should be investigated by theoretical computer scientists in order to establish their computational and complexity theoretic properties. (What about LEGO bricks as a non-uniform computing model?)

Update (22 June 2012)
---------------------

Here is &lt;a href=&quot;http://www.legoturingmachine.org&quot;&gt;another Turing machine&lt;/a&gt; built at Centrum Wiskunde &amp; Informatica (Amsterdam) “in honor of the Alan Turing year 2012”:

&lt;iframe src=&quot;https://player.vimeo.com/video/44202270&quot; width=&quot;640&quot; height=&quot;272&quot; frameborder=&quot;0&quot; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;

And what about circuits? &lt;a href=&quot;http://goldfish.ikaruga.co.uk/logic.html&quot;&gt;Here&lt;/a&gt; is how to construct mechanical (“push-pull”) logic gates. For instance, this is an AND gate:

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/E_i6TEFWbtw&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;</content><author><name>Antonio E. Porreca</name><email>antonio.porreca@lis-lab.fr</email></author><summary type="html">There is a LEGO Turing machine, constructed at Aarhus University (see here for more information):</summary></entry><entry><title type="html">On a philosophical quest</title><link href="https://aeporreca.org/blog/philosophical-quest/" rel="alternate" type="text/html" title="On a philosophical quest" /><published>2010-12-02T00:00:00+01:00</published><updated>2010-12-02T00:00:00+01:00</updated><id>https://aeporreca.org/blog/philosophical-quest</id><content type="html" xml:base="https://aeporreca.org/blog/philosophical-quest/">&lt;em&gt;This was originally posted as an answer to the question &lt;a href=&quot;http://cstheory.stackexchange.com/questions/1562/why-go-to-theoretical-computer-science-research/&quot;&gt;Why go to theoretical computer science/research?&lt;/a&gt; on the &lt;a href=&quot;http://cstheory.stackexchange.com&quot;&gt;Theoretical Computer Science&lt;/a&gt; Stack Exchange website.&lt;/em&gt;

One of the main reasons why I find the theory of computation (“my” branch of theoretical computer science) fascinating and worth studying is the following one: it provides us with a way to investigate some deep (and sometimes puzzling) philosophical questions.

One of the founders of the theory of computation, Alan Turing, tried to pin down the meaning of “computing a function” for a human being equipped with a piece of paper, by giving a mathematical description of the process. I’m not the only one to think he was extremely successful, and Turing machines proved to be an accurate model of many other computing processes.

Now that we possess a class of mathematical objects describing computations, we can actually prove theorems about them, thus trying to uncover what can be computed, and how it can be computed; it immediately turned out that lots of perfectly legitimate functions cannot be computed at all, and that they can be classified according to a degree of uncomputability (some functions are just “more uncomputable” than others).

Some other guys, the first ones usually identified with Juris Hartmanis and Richard E. Stearns, tried to describe mathematically what it means for a function (resp., a problem) to be &lt;em&gt;hard&lt;/em&gt; or &lt;em&gt;easy&lt;/em&gt; to compute (resp., to solve). There are several &lt;em&gt;complexity measures&lt;/em&gt; according to which the hardness of problems can be described; the most common one is just how much time we need to solve them. Alan Cobham and Jack Edmonds were quite successful in identifying a reasonable notion of “efficient computation”.

Within the computational complexity framework, we can now &lt;em&gt;prove&lt;/em&gt; some results that are consistent with our intuitive notion of computation. My favourite example is the time hierarchy theorem: if we are given more time to compute, we can solve harder problems.

The central open problem of complexity theory, &lt;strong&gt;P&lt;/strong&gt; vs &lt;strong&gt;NP&lt;/strong&gt;, is just a formalisation of another philosophically significant question: is it really harder to solve a problem than to check if an alleged solution of it is indeed correct? I believe this question is worth asking, and answering, independently of its practical significance.</content><author><name>Antonio E. Porreca</name><email>antonio.porreca@lis-lab.fr</email></author><summary type="html">One of the main reasons why I find the theory of computation (“my” branch of theoretical computer science) fascinating and worth studying is the following one: it provides us with a way to investigate some deep (and sometimes puzzling) philosophical questions.</summary></entry><entry><title type="html">Developments in Language Theory 2010: Where tetration has lease</title><link href="https://aeporreca.org/blog/dlt-2010-where-tetration-has-lease/" rel="alternate" type="text/html" title="Developments in Language Theory 2010: Where tetration has lease" /><published>2010-08-15T00:00:00+02:00</published><updated>2010-08-15T00:00:00+02:00</updated><id>https://aeporreca.org/blog/dlt-2010-where-tetration-has-lease</id><content type="html" xml:base="https://aeporreca.org/blog/dlt-2010-where-tetration-has-lease/">Tomorrow I'm leaving for Canada, to attend the &lt;a href=&quot;http://www.csd.uwo.ca/DLT2010/&quot;&gt;14th International Conference on Developments in Language Theory&lt;/a&gt;, held at &lt;a href=&quot;http://uwo.ca/&quot;&gt;University of Western Ontario&lt;/a&gt;, London. You can find the programme &lt;a href=&quot;http://www.csd.uwo.ca/DLT2010/program.html&quot;&gt;here&lt;/a&gt;.

Our &lt;abbr&gt;DLT&lt;/abbr&gt; 2010 paper, authored by yours truly, Alberto Leporati and Claudio Zandron, is titled &lt;a href=&quot;https://doi.org/10.1007/978-3-642-14455-4_33&quot;&gt;On a powerful class of non-universal P systems with active membranes&lt;/a&gt;. In it, we describe a restricted variant of &lt;a href=&quot;http://researchspace.auckland.ac.nz/handle/2292/3611&quot;&gt;P systems with active membranes&lt;/a&gt; (computing devices inspired by biological cells) that is not Turing-equivalent, but nonetheless decide a surprisingly large class of languages. This class also seems to be somewhat &quot;new&quot; (we haven't been able to find any reference to it in the literature).

Consider the operation of &lt;a href=&quot;http://en.wikipedia.org/wiki/Tetration&quot;&gt;&lt;em&gt;tetration&lt;/em&gt;&lt;/a&gt; (iterated exponentiation), defined as

$${}^n b = \underbrace{\,b^{b^{b^{\cdot^{\cdot^{\cdot^b}}}}}}_{n \text{ times}}$$

and call &lt;strong&gt;PTETRA&lt;/strong&gt; the class of languages decided by Turing machines working in time ${}^{p(n)}2$ for some polynomial &lt;em&gt;p&lt;/em&gt; (the &lt;strong&gt;P&lt;/strong&gt; in &lt;strong&gt;PTETRA&lt;/strong&gt; signifies the fact that exponentiation is iterated only polynomially many times instead of, say, exponentially many). This class is trivially closed under union, intersection, complement, and polytime reductions. Furthermore, it's easy to see that it can also be defined in terms of &lt;em&gt;space&lt;/em&gt; ${}^{p(n)}2$ (you can also throw in nondeterminism, if you really want to).

Well, the main result of our paper is the following funny theorem.

&lt;strong&gt;Theorem.&lt;/strong&gt; &lt;em&gt;The class of languages decided by our variant of P system with active membranes is exactly&lt;/em&gt; &lt;strong&gt;PTETRA&lt;/strong&gt;.

The proof works like this:
&lt;ul&gt;
&lt;li&gt;The upper bound is proved by a few calculations that show how our P systems can never use more that tetrational space, because the process of membrane division (which generates lots of new membranes) must stop after a finite number of steps.&lt;/li&gt;
&lt;li&gt;To prove the lower bound, we show that we can simulate with our P systems all tetrational-space &lt;a href=&quot;http://en.wikipedia.org/wiki/Counter_machine&quot;&gt;counter machines&lt;/a&gt;, hence tetrational-space Turing machines.&lt;/li&gt;
&lt;/ul&gt;

The result is somewhat unexpected, as I was under the impression that all we could do with these P systems was &lt;strong&gt;EXPSPACE&lt;/strong&gt;, and indeed I wasted a few days trying to prove that. Fortunately, reality turned out to be much more interesting.</content><author><name>Antonio E. Porreca</name><email>antonio.porreca@lis-lab.fr</email></author><summary type="html">Tomorrow I’m leaving for Canada, to attend the 14th International Conference on Developments in Language Theory, held at University of Western Ontario, London. You can find the programme here.</summary></entry><entry><title type="html">A relatively serious proof that P ≠ NP?</title><link href="https://aeporreca.org/blog/proof-that-p-isnt-np/" rel="alternate" type="text/html" title="A relatively serious proof that P &amp;ne; NP?" /><published>2010-08-09T00:00:00+02:00</published><updated>2010-08-09T00:00:00+02:00</updated><id>https://aeporreca.org/blog/proof-that-p-isnt-np</id><content type="html" xml:base="https://aeporreca.org/blog/proof-that-p-isnt-np/">&lt;em&gt;Note: I’ve stopped updating this post, now that I’ve collected quite a lot of comments. For the latest news, I suggest checking &lt;a href=&quot;http://twitter.com/aeporreca&quot;&gt;my tweets&lt;/a&gt; and, in particular, the &lt;a href=&quot;http://michaelnielsen.org/polymath1/index.php?title=Deolalikar%27s_P!%3DNP_paper&quot;&gt;Polymath wiki page&lt;/a&gt; (edited by more people than just yours truly). Maybe I’ll have another post in the next few days, if something shocking happens.&lt;/em&gt;

The news seems to have originated from a &lt;a href=&quot;http://gregbaker.ca/blog/2010/08/07/p-n-np/&quot;&gt;post&lt;/a&gt; on Greg Baker’s blog. The author of the claimed proof of P ≠ NP is &lt;a href=&quot;http://www.hpl.hp.com/personal/Vinay_Deolalikar/&quot;&gt;Vinay Deolalikar&lt;/a&gt;, principal research scientist at HP Labs (though &quot;[t]his work was pursued independently of [his] duties as a HP Labs researcher, and without the knowledge of others&quot;). Judging from his past publication history, we can’t afford to immediately dismiss the author as a crank this time.

It seems that a preliminary version of the &lt;a href=&quot;http://www.hpl.hp.com/personal/Vinay_Deolalikar/Papers/pnp_preliminary.pdf&quot;&gt;paper&lt;/a&gt;, dated 6 August (note: there’s a new version &lt;del datetime=&quot;2010-08-10T18:52:54+00:00&quot;&gt;with &quot;minor updates&quot; &lt;a href=&quot;http://www.hpl.hp.com/personal/Vinay_Deolalikar/Papers/pnp_updated.pdf&quot;&gt;here&lt;/a&gt;&lt;/del&gt; &lt;a href=&quot;http://www.hpl.hp.com/personal/Vinay_Deolalikar/Papers/pnp_updated-mjr.pdf&quot;&gt;here&lt;/a&gt;), was circulated by Deolalikar to &quot;several leading researchers in various areas&quot;, rather amusingly in both &quot;10pt and 12pt fonts&quot; (apparently, the proof does not relativise to 11pt). According to Baker, &lt;a href=&quot;http://www.cs.toronto.edu/~sacook/&quot;&gt;Stephen Cook&lt;/a&gt; commented &quot;This appears to be a relatively serious claim to have solved P vs NP&quot;. The story started spreading when someone uploaded the paper to &lt;a href=&quot;http://www.scribd.com/doc/35539144/pnp12pt&quot;&gt;Scribd&lt;/a&gt; without the author’s knowledge.

The paper is very long (&lt;del datetime=&quot;2010-08-09T19:55:39+00:00&quot;&gt;103&lt;/del&gt; &lt;del datetime=&quot;2010-08-10T18:56:17+00:00&quot;&gt;104&lt;/del&gt; 107 pages in the 12pt version) and involves &quot;several fields spanning logic, statistics, graphical models, random ensembles, and statistical physics&quot;. So, before engaging in a serious reading, I’m going to wait for someone more qualified than me to make a preliminary evaluation. In particular, the word &quot;physics&quot; sounds pretty scary to me in this context.

What do computer scientists and related people on the web think about it? Here is a list of people who commented on Deolalikar’s paper (until 10 August).

- Richard Lipton &lt;a href=&quot;http://rjlipton.wordpress.com/2010/08/08/a-proof-that-p-is-not-equal-to-np/&quot;&gt;thinks&lt;/a&gt; this is a &quot;well written paper, by a serious researcher&quot; and &quot;who knows—perhaps this is the solution we have all have been waiting for&quot;.
- Lance Fortnow is &lt;a href=&quot;http://twitter.com/fortnow/status/20673954168&quot;&gt;laconically cautious&lt;/a&gt;: &quot;Much ado about an unverified proof&quot;.
- Dave Bacon &lt;a href=&quot;http://dabacon.org/pontiff/?p=4286&quot;&gt;thinks&lt;/a&gt; &quot;this a fairly serious attack by a serious researcher&quot;.
- Scott Aaronson is willing to supplement the million dollar Clay Millennium prize by $200,000 if the proof turns out to be correct, though he &lt;a href=&quot;http://scottaaronson.com/blog/?p=456&quot;&gt;remarks&lt;/a&gt; that &quot;even if this attempt fails, it seems to introduce some thought-provoking new ideas&quot;.
- Daniel Lemire doesn’t &lt;a href=&quot;http://www.daniel-lemire.com/blog/archives/2010/08/09/how-to-get-everyone-talking-about-your-research/&quot;&gt;comment&lt;/a&gt; directly on the contents the paper, but on some reasons why Deolalikar gets so much attention: well-written paper, and enough reputation to be considered as a peer.
- Suresh Venkatasubramanian seems to &lt;a href=&quot;http://twitter.com/geomblog/status/20713035743&quot;&gt;think&lt;/a&gt; that understanding this paper does indeed require knowledge of many fields. He’s also &lt;a href=&quot;http://geomblog.blogspot.com/2010/08/on-deolalikar-proof-crowdsourcing.html&quot;&gt;proposing&lt;/a&gt; a collaborative effort to identify any issue with the current version of the proof.
- William Gasarch would &lt;a href=&quot;http://blog.computationalcomplexity.org/2010/08/that-p-ne-np-proof-whats-up-with-that.html&quot;&gt;bet&lt;/a&gt; that Deolalikar &quot;proved something very interesting, but not P ≠ NP&quot;: we just haven’t made enough progress on that problem yet.
- Bruce Schneier also &lt;a href=&quot;http://www.schneier.com/blog/archives/2010/08/p_np_1.html&quot;&gt;bets&lt;/a&gt; that the proof is flawed, despite the author being a &quot;respected researcher&quot;.
- Ryan Williams is quite &lt;a href=&quot;http://twitter.com/rrwilliams/status/20741046788&quot;&gt;sceptical&lt;/a&gt; (his technical comments &lt;a href=&quot;http://twitter.com/rrwilliams/status/20741170730&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;http://twitter.com/rrwilliams/status/20741266130&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;http://twitter.com/rrwilliams/status/20741316229&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://twitter.com/rrwilliams/status/20741339603&quot;&gt;here&lt;/a&gt;).
- Richard Lipton and Ken Regan &lt;a href=&quot;http://rjlipton.wordpress.com/2010/08/09/issues-in-the-proof-that-p%E2%89%A0np/&quot;&gt;highlight&lt;/a&gt; some technical issues with the proof.
- In the mean time, Geomblog &lt;a href=&quot;http://geomblog.blogspot.com/2010/08/polymath-home-for-analysis-of.html&quot;&gt;reports&lt;/a&gt; that the analysis of Deolalikar’s proof moved to a &lt;a href=&quot;http://michaelnielsen.org/polymath1/index.php?title=Deolalikar%27s_P%21%3DNP_paper&quot;&gt;new wiki&lt;/a&gt;, in collaboration with the Polymath project.
- Terence Tao &lt;a href=&quot;http://www.google.com/buzz/114134834346472219368/1vfSCPtRQZf/Vinay-Deolaikar-recently-released-a-102-page&quot;&gt;says&lt;/a&gt; &quot;[t]his is a serious effort, but one which sits somewhat orthogonally to the existing work on this problem&quot;, so a sensible evaluation might require quite some time.
- András Salamon is also &quot;&lt;a href=&quot;http://constraints.wordpress.com/2010/08/09/deolalikars-manuscript/&quot;&gt;sceptical&lt;/a&gt;&quot;, as &quot;the technique used is unlikely to work to prove the desired result&quot;. However, &quot;the paper is definitely worth reading&quot; anyway.
- More detailed &lt;a href=&quot;http://twitter.com/rrwilliams/status/20805879147&quot;&gt;comments&lt;/a&gt; by Ryan Williams on the &lt;a href=&quot;http://michaelnielsen.org/polymath1/index.php?title=Deolalikar%27s_P!%3DNP_paper#Issues_with_random_k-SAT&quot;&gt;Polymath wiki&lt;/a&gt;.
- In another &lt;a href=&quot;http://dabacon.org/pontiff/?p=4292&quot;&gt;post&lt;/a&gt;, Dave Bacon comments on the reasons behind the interest drawn by the paper (it’s very broad in scope and contains a lot of review). And, by the way, he and his colleagues are sceptical too.
- Charanjit Jutla &lt;a href=&quot;http://rjlipton.wordpress.com/2010/08/09/issues-in-the-proof-that-p%E2%89%A0np/#comment-4712&quot;&gt;thinks&lt;/a&gt; there’s a &quot;huge (unfixable) gap&quot;.
- As of 10 August, 22:58 UTC there isn’t any mention of the P ≠ NP paper on Deolalikar’s &lt;a href=&quot;http://www.hpl.hp.com/personal/Vinay_Deolalikar/&quot;&gt;home page&lt;/a&gt; anymore (though the paper itself is still available in its &lt;a href=&quot;http://www.hpl.hp.com/personal/Vinay_Deolalikar/Papers/pnp_updated-mjr.pdf&quot;&gt;third version&lt;/a&gt;).

The proof also made it to Gerhard Woeginger’s &lt;a href=&quot;http://www.win.tue.nl/~gwoegi/P-versus-NP.htm&quot;&gt;P-versus-NP page&lt;/a&gt; and, somehow, to &lt;a href=&quot;http://isohunt.com/torrent_details/204525141/?tab=summary&quot;&gt;BitTorrent&lt;/a&gt; (?). &lt;em&gt;Nature news&lt;/em&gt; has an article on this story: &lt;a href=&quot;http://www.nature.com/news/2010/100810/full/news.2010.398.html&quot;&gt;Million-dollar problem cracked?&lt;/a&gt;.

On a side note, I’m genuinely surprised by the amount of &lt;a href=&quot;http://search.twitter.com/search?q=P%20NP&quot;&gt;buzz&lt;/a&gt; on Twitter generated by this story (mostly deriving from &lt;a href=&quot;http://science.slashdot.org/story/10/08/08/226227/Claimed-Proof-That-P--NP&quot;&gt;Slashdot&lt;/a&gt; and &lt;a href=&quot;http://news.ycombinator.net/item?id=1585850&quot;&gt;Hacker News&lt;/a&gt; I think). Independently of the actual correctness of the proof, I believe this is a good thing, as it suggests that there’s enough public interest for the most important open problem in computer science.</content><author><name>Antonio E. Porreca</name><email>antonio.porreca@lis-lab.fr</email></author><summary type="html">The news seems to have originated from a [post](http://gregbaker.ca/blog/2010/08/07/p-n-np/) on Greg Baker’s blog. The author of the claimed proof of P ≠ NP is [Vinay Deolalikar](http://www.hpl.hp.com/personal/Vinay_Deolalikar/), principal research scientist at HP Labs (though “[t]his work was pursued independently of [his] duties as a HP Labs researcher, and without the knowledge of others”). Judging from his past publication history, we can’t afford to immediately dismiss the author as a crank this time.</summary></entry></feed>