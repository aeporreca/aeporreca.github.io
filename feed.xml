<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2017-12-04T17:44:46+01:00</updated><id>http://localhost:4000/</id><title type="html">Antonio E. Porreca</title><subtitle>Antonio E. Porreca’s Website</subtitle><author><name>Antonio E. Porreca</name><email>porreca@disco.unimib.it</email></author><entry><title type="html">Nash beats Gödel: On the history of complexity and cryptography</title><link href="http://localhost:4000/blog/nash-beats-godel/" rel="alternate" type="text/html" title="Nash beats Gödel: On the history of complexity and cryptography" /><published>2012-02-17T00:00:00+01:00</published><updated>2012-02-17T00:00:00+01:00</updated><id>http://localhost:4000/blog/nash-beats-godel</id><content type="html" xml:base="http://localhost:4000/blog/nash-beats-godel/">We know that Kurt Gödel, unhappy with having only completeness and incompleteness theorems named after him, also essentially invented the &lt;strong&gt;P&lt;/strong&gt; vs &lt;strong&gt;NP&lt;/strong&gt; question in a &lt;a href=&quot;http://rjlipton.wordpress.com/the-gdel-letter/&quot;&gt;1956 letter to John Von Neumann&lt;/a&gt;. Unfortunately, there were no blogs back then, so we had to wait until the 1980s to read it, and Cook, Karp &amp; co. had to reinvent the question from scratch.

What we &lt;em&gt;didn't&lt;/em&gt; know until a few days ago (I personally discovered it today via &lt;a href=&quot;http://aaronsadventures.blogspot.com/2012/02/amazing-new-declassified-document.html&quot;&gt;Aaron Roth&lt;/a&gt;) is that in 1955, one year before Gödel's letter, &lt;em&gt;John Nash&lt;/em&gt; also had something to say on the topic, in a few letters to NSA. Gödel and Nash independently made &lt;em&gt;strikingly&lt;/em&gt; similar remarks, starting from completely different points of view. While Gödel's perspective was that of a logician (his letter was about the length of proofs in first-order logic), Nash was interested in cryptography, and wrote about the security of cryptographic systems.

&lt;a href=&quot;http://www.nsa.gov/public_info/press_room/2012/nash_exhibit.shtml&quot;&gt;NSA's press release&lt;/a&gt; about the declassification of Nash's letters was apparently made on 27 January 2012; &lt;a href=&quot;http://www.nsa.gov/public_info/_files/nash_letters/nash_letters1.pdf&quot;&gt;here&lt;/a&gt; is a PDF containing Nash's letters and the replies he got from NSA. I'll only focus on the second letter here, leaving a cryptoanalysis of the system he proposed in the third one to someone more knowledgeable than me on the topic.

I assume Nash was familiar with Shannon's work &lt;em&gt;Communication theory of secrecy systems&lt;/em&gt; (1949), so he already knew that the only mathematically unbreakable cipher is one-time pad. Hence, in order to discuss practical encryption systems, he invented the principles of modern cryptography about 20 years in advance:

&lt;blockquote style=&quot;border:none;&quot;&gt;If &lt;span style=&quot;font-style:normal;&quot;&gt;[&lt;/span&gt;the computation required in order to recover the key from the ciphertext&lt;span style=&quot;font-style:normal;&quot;&gt;]&lt;/span&gt;, although possible in principle, were sufficiently long at best then the process could still be secure in a practical sense.&lt;/blockquote&gt;

Here is some computational complexity, 8 years before Hartmanis and Stearns:

&lt;blockquote style=&quot;border:none;&quot;&gt;The most direct computation procedure would be for the enemy to try all &lt;span style=&quot;font-style:normal;&quot;&gt;2&lt;/span&gt;&lt;sup&gt;&lt;em&gt;r&lt;/em&gt;&lt;/sup&gt; possible keys, one by one. Obviously this is &lt;span style=&quot;font-style:normal;&quot;&gt;easily&lt;/span&gt; made impractical for the enemy by simply choosing &lt;em&gt;r&lt;/em&gt; large enough.&lt;/blockquote&gt;

Apparently Nash already knew quite well (&quot;obviously&quot;, &quot;easily&quot;) that exponential time is too much.

And here is how one can classify cryptosystems (but also general computation problems) according to him. I think you can read essentially the same paragraph in all modern complexity theory and cryptography books.

&lt;blockquote style=&quot;border:none;&quot;&gt;So a logical way to classify enciphering processes is by the way in which the computation length for the computation of the key increases with increasing length of the key. This is at best exponential and at worst probably a relatively small power of &lt;em&gt;r&lt;/em&gt;, &lt;em&gt;ar&lt;/em&gt;&lt;sup&gt;&lt;span style=&quot;font-style:normal;&quot;&gt;2&lt;/span&gt;&lt;/sup&gt; or &lt;em&gt;ar&lt;/em&gt;&lt;sup&gt;&lt;span style=&quot;font-style:normal;&quot;&gt;3&lt;/span&gt;&lt;/sup&gt;, as in substitution ciphers.&lt;/blockquote&gt;

Nash also conjectures that many problems cannot be solved in polynomial time. In hindsight, there is some naivety here about the design of good ciphers, but this is still very interesting:

&lt;blockquote style=&quot;border:none;&quot;&gt;Now my general conjecture is as follows: For almost all sufficiently complex types of enciphering, especially where the instructions given by different portions of the key interact complexly with each other in the determination of their ultimate effects on the enciphering, the mean key computation length increases exponentially with the length of the key, or in other words, with the information content of the key.

The significance of this general conjecture, assuming its truth, is easy to see. It means that it is quite feasible to design ciphers that are effectively unbreakable. As ciphers became more sophisticated the game of cipher breaking by skilled teams, etc., should become a thing of the past.&lt;/blockquote&gt;

All in all, this seems to me a truly magnificent historical document, possibly on par with the aforementioned letter by Gödel. I'd really like to know what people working in cryptography think about this.

These &quot;Lost Letter&quot; discoveries also make me wonder what further treasures may be hidden in NSA's cardboard boxes, or in private correspondence collections. What if there exists a “Fermat's Lost (Last?) Letter” along the lines of &quot;Here's the proof that didn't fit in the margin&quot;, or a “Russell's Lost Letter” that he sent to Whitehead to tell him “I found a proposition that can't be proved nor refuted in &lt;em&gt;Principia Mathematica&lt;/em&gt;”?</content><author><name>Antonio E. Porreca</name><email>porreca@disco.unimib.it</email></author><summary type="html">We know that Kurt Gödel, unhappy with having only completeness and incompleteness theorems named after him, also essentially invented the P vs NP question in a 1956 letter to John Von Neumann. Unfortunately, there were no blogs back then, so we had to wait until the 1980s to read it, and Cook, Karp &amp;amp; co. had to reinvent the question from scratch.</summary></entry><entry><title type="html">Do waterfalls play chess? and other stories</title><link href="http://localhost:4000/blog/do-waterfalls-play-chess/" rel="alternate" type="text/html" title="Do waterfalls play chess? and other stories" /><published>2011-08-13T00:00:00+02:00</published><updated>2011-08-13T00:00:00+02:00</updated><id>http://localhost:4000/blog/do-waterfalls-play-chess</id><content type="html" xml:base="http://localhost:4000/blog/do-waterfalls-play-chess/">I finally took the time to read &lt;a href=&quot;http://www.scottaaronson.com/&quot;&gt;Scott Aaronson&lt;/a&gt;’s new paper &lt;a href=&quot;http://eccc.hpi-web.de/report/2011/108/&quot;&gt;Why philosophers should care about computational complexity&lt;/a&gt; (there's a &lt;a href=&quot;http://www.scottaaronson.com/blog/?p=735&quot;&gt;post&lt;/a&gt; about it with lots of comments on his &lt;a href=&quot;http://www.scottaaronson.com/blog/&quot;&gt;blog&lt;/a&gt;).

Summary: Aaronson, knowing that I’m writing my thesis at the moment, took the liberty of preparing an introduction for me. Thanks! I'm going to include it verbatim. (Just kidding of course, though I'm actually going to cite this paper extensively.)

&lt;a href=&quot;http://www.flickr.com/photos/phalaenopsisaphrodite523/5148870343/&quot; title=&quot;Red Queen by Phalaenopsis Aphrodite, on Flickr&quot;&gt;&lt;img src=&quot;http://farm5.static.flickr.com/4008/5148870343_08cc5cf697.jpg&quot; alt=&quot;Red Queen&quot; class=&quot;aligncenter size-full wp-image-863&quot;&gt;&lt;/a&gt;

The early work on computability had a lot of philosophical significance; actually, we can say that computability was &lt;em&gt;born&lt;/em&gt; to answer a philosophical question, Hilbert's &lt;a href=&quot;http://en.wikipedia.org/wiki/Entscheidungsproblem&quot;&gt;&lt;em&gt;Entscheidungsproblem&lt;/em&gt;&lt;/a&gt;. We all know that the answer turned out to be negative (or do we? See Section 3.1). Apparently computational complexity theory, the theory of computing under limited resources, didn't turn out to be so popular among philosophers. That's &lt;a href=&quot;http://aeporreca.org/2010/12/02/philosophical-quest/&quot;&gt;obviously&lt;/a&gt; a problem, and this paper tries to do something about it.

After a brief introduction to complexity theory (Section 2), Aaronson turns his attention to one of the main cornerstones of this field, which is also one the points that are usually criticised: the relevance of polynomial time, as opposed to exponential time. Here he argues that this distinction is at least as interesting as the distinction between computable and uncomputable. Section 3.3 contains an interesting question that can be answered using a complexity-theoretic argument: why would we call &lt;a href=&quot;http://en.wikipedia.org/wiki/Mersenne_prime&quot;&gt;2&lt;sup&gt;43112609&lt;/sup&gt; &amp;minus; 1&lt;/a&gt; (together with a proof of its primality) a “known” prime, while “the first prime large than 2&lt;sup&gt;43112609&lt;/sup&gt; &amp;minus; 1” feels somehow “unknown”?

Section 4 is about the &lt;a href=&quot;http://en.wikipedia.org/wiki/Turing_test&quot;&gt;Turing test&lt;/a&gt;. This is the first time I see in print what Aaronson calls the “lookup-table argument” (though he cites &lt;a href=&quot;http://philpapers.org/rec/BLOSAA&quot;&gt;Ned Block&lt;/a&gt; and others for it): namely, that there’s no doubt whatsoever that an abstract program &lt;em&gt;P&lt;/em&gt; able to pass the Turing test does actually exist (you just need a huge but finite table of replies dependent on the previous history of the conversation). The only way to claim the impossibility of a machine passing the Turing test, apart from metaphysical arguments, requires addressing concerns such as the efficiency of &lt;em&gt;P&lt;/em&gt; or the availability of enough space in the universe to store it. That is, complexity-theoretic questions.

Section 5, possibly the most interesting one, addresses the problem of logical omniscience. It is usually assumed that knowledge is closed under deduction rules; for instance, if I know &lt;em&gt;A&lt;/em&gt;, and I know &lt;em&gt;B&lt;/em&gt;, then I certainly know &lt;em&gt;A&lt;/em&gt; &amp;and; &lt;em&gt;B&lt;/em&gt;. But consider this: while I know enough basic axioms of maths, I surely don’t &lt;em&gt;know&lt;/em&gt; that Fermat’s last theorem holds (except by trusting the mathematical community, which I usually do), even though it’s a mere deductive consequence of said axioms. We can argue that human beings do not possess the computational resources needed to actually achieve omniscience.

Section 6 is about a “waterfall argument” against &lt;a href=&quot;http://en.wikipedia.org/wiki/Computational_theory_of_mind&quot;&gt;computationalism&lt;/a&gt;. The idea is that the meaning of what a supposed computer actually computes is always imposed by someone looking at it, that is, it’s always &lt;em&gt;relative to some external observer&lt;/em&gt;. I freely admit I am (was?) a fan of this argument, though I don’t necessarily see it as an attack to computationalism (I use to call this “computational relativism”, a term whose invention I claim, since Google returns zero hits for it :-)). According to some proponents, this leads to some “strange” consequences.

&lt;a href=&quot;http://www.flickr.com/photos/crabbylioncardsandmore/4980927493/&quot; title=&quot;Niagara Falls by Kevin Timothy, on Flickr&quot;&gt;&lt;img src=&quot;http://farm5.static.flickr.com/4084/4980927493_38dc298025.jpg&quot; width=&quot;500&quot; height=&quot;335&quot; alt=&quot;Niagara Falls&quot; class=&quot;aligncenter size-full wp-image-863&quot;&gt;&lt;/a&gt;

For instance, assume that we encode chess positions in the physical states of a waterfall, then take a look at some “final” state of the waterfall, and once again interpret that as a chess position. Can the waterfall be said to play chess? Aaronson argues that it is not so, unless the encoding (which is just a &lt;a href=&quot;http://en.wikipedia.org/wiki/Reduction_(complexity)&quot;&gt;reduction&lt;/a&gt; from chess to computing the state of the waterfall) can be computed by a procedure requiring &lt;em&gt;less&lt;/em&gt; resources than those needed to actually compute the state of the waterfall. But then a question emerges: does &lt;a href=&quot;http://dx.doi.org/10.1126/science.7973651&quot;&gt;DNA&lt;/a&gt; compute Hamilton paths?

Section 7 describes how computational complexity can help to define what inductive inference is about, including Occam’s razor and the &lt;a href=&quot;http://en.wikipedia.org/wiki/Grue_and_bleen&quot;&gt;“new riddle of induction”&lt;/a&gt;.

Then, in Section 8, Aaronson argues that complexity theory might inspire some debate about the interpretation of quantum physics, particularly about the &lt;a href=&quot;http://en.wikipedia.org/wiki/Many-worlds_interpretation&quot;&gt;many-worlds interpretation&lt;/a&gt;.

In Section 9 new notions of proof, quite distinct from the usual notion of “formal” and “informal” proofs, are described. These are all based on complexity-theoretic and cryptographic results: &lt;a href=&quot;http://en.wikipedia.org/wiki/Interactive_proof_system&quot;&gt;interactive proof systems&lt;/a&gt;, &lt;a href=&quot;http://en.wikipedia.org/wiki/Zero-knowledge_proof&quot;&gt;zero-knowledge proofs&lt;/a&gt;, &lt;a href=&quot;http://en.wikipedia.org/wiki/Probabilistically_checkable_proof&quot;&gt;probabilistically checkable proofs&lt;/a&gt;.

Section 10 is about the difference between time and space. This is usually stated as “space can be reused, time cannot”. What if we allow time travel (i.e., &lt;a href=&quot;http://en.wikipedia.org/wiki/Closed_timelike_curve&quot;&gt;closed timelike curves&lt;/a&gt;)? The assumption that &lt;strong&gt;P&lt;/strong&gt; &amp;ne; &lt;strong&gt;NP&lt;/strong&gt; might suggest an argument for the impossibility of time travel, even assuming the validity of quantum physics.

Section 11 is about economics and game theory; here complexity theory can be useful to describe the behaviour of agents under &lt;a href=&quot;http://en.wikipedia.org/wiki/Bounded_rationality&quot;&gt;bounded rationality&lt;/a&gt; assumptions.

Finally, in Section 12, Aaronson concludes that &lt;em&gt;lots&lt;/em&gt; of philosophical problems seem to have interesting complexity-theoretic aspects. Several questions remain open, and the most important of those is still “If &lt;strong&gt;P&lt;/strong&gt; &amp;ne; &lt;strong&gt;NP&lt;/strong&gt;, then how have humans managed to make such enormous mathematical progress, even in the face of the general intractability of theorem-proving?”.

I really hope that Aaronson’s paper spurs a lot of philosophical discussion involving and concerning complexity theory; I too believe there’s much to write about that.</content><author><name>Antonio E. Porreca</name><email>porreca@disco.unimib.it</email></author><summary type="html">I finally took the time to read Scott Aaronson’s new paper Why philosophers should care about computational complexity (there’s a post about it with lots of comments on his blog).</summary></entry><entry><title type="html">Time is unpredictable</title><link href="http://localhost:4000/blog/time-is-unpredictable/" rel="alternate" type="text/html" title="Time is unpredictable" /><published>2011-02-19T00:00:00+01:00</published><updated>2011-02-19T00:00:00+01:00</updated><id>http://localhost:4000/blog/time-is-unpredictable</id><content type="html" xml:base="http://localhost:4000/blog/time-is-unpredictable/">*This post is inspired by the question [Are runtime bounds in P decidable?](http://cstheory.stackexchange.com/q/5004/182) asked by [John Sidles](http://www.mrfm.org/) on the CSTheory website, and the [answer](http://cstheory.stackexchange.com/questions/5004/are-runtime-bounds-in-p-decidable-answer-no/5006#5006) given by [Emanuele Viola](http://www.ccs.neu.edu/home/viola/).*

Is there an automatic procedure to determine whether a given Turing machine, *known* to be halting, operates within time bound $O(f)$ (assuming $f$ is a computable function)?

Predictably, the answer turns out to be negative.

Let’s start by formalising the problem. Assume $M$ is the Turing machine whose runtime we’re interested in, and $F$ is another TM computing the time bound $f$; then

$$L = \{ (M, F) : M \text{ halts within } O(F) \text{ steps} \}.$$

Also let $H$ be the halting problem:

$$H = \{ (M’, x) : M \text{ halts on input } x \}.$$

We can now define the function $R(M’, x) = (M, F)$, where $F$ computes $n^2$ and $M$, on input $y$, behaves as follows:

- Simulate $M'$ on $x$ for $n = \lvert y \rvert$ steps.
- If $M'$ has already halted, then loop for $n^2$ steps.
- Otherwise, loop for $n^3$ steps.

We’re finally ready to prove our undecidability result.

**Theorem.** $R$ is a many-one reduction from $H$ to $L$.

*Proof.* Clearly $R$ is a computable function, so we just need to show that

$$(M', x) \in H \iff (M, F) \in L.$$

If  $(M', x) \in H$, that is, if $M'$ halts on input $x$, then it does so in $k$ steps (for some $k \in \mathbb{N}$). Hence $M$ runs in $O(n^2)$ time (notice that it runs in $n^3$ time for $\lvert y \rvert &lt; k$, but it's only the asymptotic behaviour that matters for us). Thus $(M, F) \in L$.

On the other hand, if $M'$ does not halt on $x$, then $M$ never completes its simulation, and the runtime for $M$ is $O(n^3)$. Thus $(M, F) \notin L$. □

**Remark.** We’ve actually proved a stronger statement, i.e., that the set of Turing machines running in $O(n^2)$ time is undecidable, even if we restrict the domain to machines halting in polynomial time. Similar undecidability results hold for an infinite class of time bounds.</content><author><name>Antonio E. Porreca</name><email>porreca@disco.unimib.it</email></author><summary type="html">This post is inspired by the question Are runtime bounds in P decidable? asked by John Sidles on the CSTheory website, and the answer given by Emanuele Viola.</summary></entry><entry><title type="html">Merry Christmath!</title><link href="http://localhost:4000/blog/merry-christmath/" rel="alternate" type="text/html" title="Merry Christmath!" /><published>2011-01-04T00:00:00+01:00</published><updated>2011-01-04T00:00:00+01:00</updated><id>http://localhost:4000/blog/merry-christmath</id><content type="html" xml:base="http://localhost:4000/blog/merry-christmath/">&lt;blockquote style=&quot;border:none;&quot;&gt;
On the second day of Christmas my true love gave to me…
The only even prime and the absolute value of &lt;em&gt;e&lt;/em&gt; to the &lt;em&gt;iπ&lt;/em&gt;.
&lt;/blockquote&gt;

A short post, just because I want this on my blog: a beautiful &lt;a href=&quot;http://vihart.com/blog/gauss/&quot;&gt;video&lt;/a&gt; and &lt;a href=&quot;http://vihart.com/music/gauss12days.mp3&quot;&gt;song&lt;/a&gt; by &lt;a href=&quot;http://vihart.com&quot;&gt;Vi Hart&lt;/a&gt; (take a look at the rest of her website, it’s amazing). Epsilon greater-than to you too.

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/sxnX5_LbBDU&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;

Also, thanks to &lt;a href=&quot;http://divisbyzero.com/&quot;&gt;Dave Richeson&lt;/a&gt; for &lt;a href=&quot;http://twitter.com/divbyzero/status/22311164692140032&quot;&gt;sharing&lt;/a&gt; it in the first place.</content><author><name>Antonio E. Porreca</name><email>porreca@disco.unimib.it</email></author><summary type="html">On the second day of Christmas my true love gave to me… The only even prime and the absolute value of e to the iπ.</summary></entry><entry><title type="html">Undecidability in terms of complexity</title><link href="http://localhost:4000/blog/undecidability-as-complexity/" rel="alternate" type="text/html" title="Undecidability in terms of complexity" /><published>2010-12-13T00:00:00+01:00</published><updated>2010-12-13T00:00:00+01:00</updated><id>http://localhost:4000/blog/undecidability-as-complexity</id><content type="html" xml:base="http://localhost:4000/blog/undecidability-as-complexity/">In his classic book &lt;a href=&quot;http://books.google.com/books?id=JogZAQAAIAAJ&quot;&gt;&lt;em&gt;Computational Complexity&lt;/em&gt;&lt;/a&gt;, Papadimitriou writes (page 59) that

&gt; Undecidability is in some sense the most lethal form of complexity.

I’ve &lt;a href=&quot;http://mathoverflow.net/questions/8632/cut-elimination/8634#8634&quot;&gt;just come across&lt;/a&gt; a paper with a very interesting remark along the same lines: &lt;a href=&quot;http://www.ams.org/journals/bull/1997-34-02/S0273-0979-97-00715-5/&quot;&gt;Making proofs without Modus Ponens: An introduction to the combinatorics and complexity of cut elimination&lt;/a&gt; by A. Carbone and S. Semmes, published in the &lt;em&gt;Bulletin of the American Mathematical Society&lt;/em&gt;. This paper is about the length of proofs (a subject I’ve already touched upon &lt;a href=&quot;http://aeporreca.org/2010/07/04/length-of-proofs/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://aeporreca.org/2010/07/12/length-of-proofs-2/&quot;&gt;here&lt;/a&gt;) without &lt;a href=&quot;http://en.wikipedia.org/wiki/Cut-elimination_theorem&quot;&gt;cut elimination&lt;/a&gt;.

On page 113, the authors write

&gt; By contrast [with propositional logic] in predicate logic one typically faces issues of algorithmic decidability or undecidability. That is, whether there is an algorithm at all that always gives the right answer, never mind how long it takes. The problem of determining whether a formula in predicate logic is a tautology is algorithmically undecidable. One can think of this as a matter of complexity, as follows. The tautologies in predicate logic of length at most $n$ form a finite set for which one can choose a finite set of proofs. Let $f(n)$ denote the maximum of the lengths of the shortest proofs of tautologies of length $\le n$. The fact that there is no algorithm for determining whether or not a formula is a tautology means that $f(n)$ grows very fast, faster than any recursive function.

Essentially, this means that we can’t design an algorithm to solve an undecidable problem because the search space is too large, and the only way to somehow bound it is via a function that grows faster than any computable function. Compare this with the search space of the “witnesses” for an &lt;strong&gt;NP&lt;/strong&gt;-complete problem: its size is exponential, which is too large to be explored efficiently (at least, as far as we know at the moment), but still small enough to make the problem decidable.</content><author><name>Antonio E. Porreca</name><email>porreca@disco.unimib.it</email></author><summary type="html">In his classic book Computational Complexity, Papadimitriou writes (page 59) that</summary></entry><entry><title type="html">Position paper: Computing with LEGO</title><link href="http://localhost:4000/blog/position-paper-computing-with-lego/" rel="alternate" type="text/html" title="Position paper: Computing with LEGO" /><published>2010-12-10T00:00:00+01:00</published><updated>2010-12-10T00:00:00+01:00</updated><id>http://localhost:4000/blog/position-paper-computing-with-lego</id><content type="html" xml:base="http://localhost:4000/blog/position-paper-computing-with-lego/">There is a LEGO Turing machine, constructed at Aarhus University (see &lt;a href=&quot;http://legoofdoom.blogspot.com&quot;&gt;here&lt;/a&gt; for more information):

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/cYw2ewoO6c4&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;

There is also a &lt;a href=&quot;http://acarol.woz.org/difference_engine.html&quot;&gt;LEGO Babbage difference engine&lt;/a&gt; built by Andrew Carol:

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/i_u3hpYMySk&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;

Now we also have an &lt;a href=&quot;http://acarol.woz.org/antikythera_mechanism.html&quot;&gt;Antikythera mechanism&lt;/a&gt; (an ancient Greek computer for predicting eclipses) made of LEGO, again by Andrew Carol:

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/RLPVCJjTNgk&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;

(Thanks to &lt;a href=&quot;http://twitter.com/CompSciFact&quot;&gt;CompSciFact&lt;/a&gt; on Twitter for the last one.)

I hereby propose that LEGO brick constructions are to be considered serious unconventional computing devices, and should be investigated by theoretical computer scientists in order to establish their computational and complexity theoretic properties. (What about LEGO bricks as a non-uniform computing model?)

Update (22 June 2012)
---------------------

Here is &lt;a href=&quot;http://www.legoturingmachine.org&quot;&gt;another Turing machine&lt;/a&gt; built at Centrum Wiskunde &amp; Informatica (Amsterdam) “in honor of the Alan Turing year 2012”:

&lt;iframe src=&quot;https://player.vimeo.com/video/44202270&quot; width=&quot;640&quot; height=&quot;272&quot; frameborder=&quot;0&quot; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;

And what about circuits? &lt;a href=&quot;http://goldfish.ikaruga.co.uk/logic.html&quot;&gt;Here&lt;/a&gt; is how to construct mechanical (“push-pull”) logic gates. For instance, this is an AND gate:

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/E_i6TEFWbtw&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;</content><author><name>Antonio E. Porreca</name><email>porreca@disco.unimib.it</email></author><summary type="html">There is a LEGO Turing machine, constructed at Aarhus University (see here for more information):</summary></entry><entry><title type="html">On a philosophical quest</title><link href="http://localhost:4000/blog/philosophical-quest/" rel="alternate" type="text/html" title="On a philosophical quest" /><published>2010-12-02T00:00:00+01:00</published><updated>2010-12-02T00:00:00+01:00</updated><id>http://localhost:4000/blog/philosophical-quest</id><content type="html" xml:base="http://localhost:4000/blog/philosophical-quest/">&lt;em&gt;This was originally posted as an answer to the question &lt;a href=&quot;http://cstheory.stackexchange.com/questions/1562/why-go-to-theoretical-computer-science-research/&quot;&gt;Why go to theoretical computer science/research?&lt;/a&gt; on the &lt;a href=&quot;http://cstheory.stackexchange.com&quot;&gt;Theoretical Computer Science&lt;/a&gt; Stack Exchange website.&lt;/em&gt;

One of the main reasons why I find the theory of computation (“my” branch of theoretical computer science) fascinating and worth studying is the following one: it provides us with a way to investigate some deep (and sometimes puzzling) philosophical questions.

One of the founders of the theory of computation, Alan Turing, tried to pin down the meaning of “computing a function” for a human being equipped with a piece of paper, by giving a mathematical description of the process. I’m not the only one to think he was extremely successful, and Turing machines proved to be an accurate model of many other computing processes.

Now that we possess a class of mathematical objects describing computations, we can actually prove theorems about them, thus trying to uncover what can be computed, and how it can be computed; it immediately turned out that lots of perfectly legitimate functions cannot be computed at all, and that they can be classified according to a degree of uncomputability (some functions are just “more uncomputable” than others).

Some other guys, the first ones usually identified with Juris Hartmanis and Richard E. Stearns, tried to describe mathematically what it means for a function (resp., a problem) to be &lt;em&gt;hard&lt;/em&gt; or &lt;em&gt;easy&lt;/em&gt; to compute (resp., to solve). There are several &lt;em&gt;complexity measures&lt;/em&gt; according to which the hardness of problems can be described; the most common one is just how much time we need to solve them. Alan Cobham and Jack Edmonds were quite successful in identifying a reasonable notion of “efficient computation”.

Within the computational complexity framework, we can now &lt;em&gt;prove&lt;/em&gt; some results that are consistent with our intuitive notion of computation. My favourite example is the time hierarchy theorem: if we are given more time to compute, we can solve harder problems.

The central open problem of complexity theory, &lt;strong&gt;P&lt;/strong&gt; vs &lt;strong&gt;NP&lt;/strong&gt;, is just a formalisation of another philosophically significant question: is it really harder to solve a problem than to check if an alleged solution of it is indeed correct? I believe this question is worth asking, and answering, independently of its practical significance.</content><author><name>Antonio E. Porreca</name><email>porreca@disco.unimib.it</email></author><summary type="html">This was originally posted as an answer to the question Why go to theoretical computer science/research? on the Theoretical Computer Science Stack Exchange website.</summary></entry><entry><title type="html">Developments in Language Theory 2010: Where tetration has lease</title><link href="http://localhost:4000/blog/dlt-2010-where-tetration-has-lease/" rel="alternate" type="text/html" title="Developments in Language Theory 2010: Where tetration has lease" /><published>2010-08-15T00:00:00+02:00</published><updated>2010-08-15T00:00:00+02:00</updated><id>http://localhost:4000/blog/dlt-2010-where-tetration-has-lease</id><content type="html" xml:base="http://localhost:4000/blog/dlt-2010-where-tetration-has-lease/">Tomorrow I'm leaving for Canada, to attend the &lt;a href=&quot;http://www.csd.uwo.ca/DLT2010/&quot;&gt;14th International Conference on Developments in Language Theory&lt;/a&gt;, held at &lt;a href=&quot;http://uwo.ca/&quot;&gt;University of Western Ontario&lt;/a&gt;, London. You can find the programme &lt;a href=&quot;http://www.csd.uwo.ca/DLT2010/program.html&quot;&gt;here&lt;/a&gt;.

Our &lt;abbr&gt;DLT&lt;/abbr&gt; 2010 paper, authored by yours truly, Alberto Leporati and Claudio Zandron, is titled &lt;a href=&quot;https://doi.org/10.1007/978-3-642-14455-4_33&quot;&gt;On a powerful class of non-universal P systems with active membranes&lt;/a&gt;. In it, we describe a restricted variant of &lt;a href=&quot;http://researchspace.auckland.ac.nz/handle/2292/3611&quot;&gt;P systems with active membranes&lt;/a&gt; (computing devices inspired by biological cells) that is not Turing-equivalent, but nonetheless decide a surprisingly large class of languages. This class also seems to be somewhat &quot;new&quot; (we haven't been able to find any reference to it in the literature).

Consider the operation of &lt;a href=&quot;http://en.wikipedia.org/wiki/Tetration&quot;&gt;&lt;em&gt;tetration&lt;/em&gt;&lt;/a&gt; (iterated exponentiation), defined as

$${}^n b = \underbrace{\,b^{b^{b^{\cdot^{\cdot^{\cdot^b}}}}}}_{n \text{ times}}$$

and call &lt;strong&gt;PTETRA&lt;/strong&gt; the class of languages decided by Turing machines working in time ${}^{p(n)}2$ for some polynomial &lt;em&gt;p&lt;/em&gt; (the &lt;strong&gt;P&lt;/strong&gt; in &lt;strong&gt;PTETRA&lt;/strong&gt; signifies the fact that exponentiation is iterated only polynomially many times instead of, say, exponentially many). This class is trivially closed under union, intersection, complement, and polytime reductions. Furthermore, it's easy to see that it can also be defined in terms of &lt;em&gt;space&lt;/em&gt; ${}^{p(n)}2$ (you can also throw in nondeterminism, if you really want to).

Well, the main result of our paper is the following funny theorem.

&lt;strong&gt;Theorem.&lt;/strong&gt; &lt;em&gt;The class of languages decided by our variant of P system with active membranes is exactly&lt;/em&gt; &lt;strong&gt;PTETRA&lt;/strong&gt;.

The proof works like this:
&lt;ul&gt;
&lt;li&gt;The upper bound is proved by a few calculations that show how our P systems can never use more that tetrational space, because the process of membrane division (which generates lots of new membranes) must stop after a finite number of steps.&lt;/li&gt;
&lt;li&gt;To prove the lower bound, we show that we can simulate with our P systems all tetrational-space &lt;a href=&quot;http://en.wikipedia.org/wiki/Counter_machine&quot;&gt;counter machines&lt;/a&gt;, hence tetrational-space Turing machines.&lt;/li&gt;
&lt;/ul&gt;

The result is somewhat unexpected, as I was under the impression that all we could do with these P systems was &lt;strong&gt;EXPSPACE&lt;/strong&gt;, and indeed I wasted a few days trying to prove that. Fortunately, reality turned out to be much more interesting.</content><author><name>Antonio E. Porreca</name><email>porreca@disco.unimib.it</email></author><summary type="html">Tomorrow I’m leaving for Canada, to attend the 14th International Conference on Developments in Language Theory, held at University of Western Ontario, London. You can find the programme here.</summary></entry><entry><title type="html">A relatively serious proof that P ≠ NP?</title><link href="http://localhost:4000/blog/proof-that-p-isnt-np/" rel="alternate" type="text/html" title="A relatively serious proof that P &amp;ne; NP?" /><published>2010-08-09T00:00:00+02:00</published><updated>2010-08-09T00:00:00+02:00</updated><id>http://localhost:4000/blog/proof-that-p-isnt-np</id><content type="html" xml:base="http://localhost:4000/blog/proof-that-p-isnt-np/">&lt;em&gt;Note: I’ve stopped updating this post, now that I’ve collected quite a lot of comments. For the latest news, I suggest checking &lt;a href=&quot;http://twitter.com/aeporreca&quot;&gt;my tweets&lt;/a&gt; and, in particular, the &lt;a href=&quot;http://michaelnielsen.org/polymath1/index.php?title=Deolalikar%27s_P!%3DNP_paper&quot;&gt;Polymath wiki page&lt;/a&gt; (edited by more people than just yours truly). Maybe I’ll have another post in the next few days, if something shocking happens.&lt;/em&gt;

The news seems to have originated from a &lt;a href=&quot;http://gregbaker.ca/blog/2010/08/07/p-n-np/&quot;&gt;post&lt;/a&gt; on Greg Baker’s blog. The author of the claimed proof of P ≠ NP is &lt;a href=&quot;http://www.hpl.hp.com/personal/Vinay_Deolalikar/&quot;&gt;Vinay Deolalikar&lt;/a&gt;, principal research scientist at HP Labs (though &quot;[t]his work was pursued independently of [his] duties as a HP Labs researcher, and without the knowledge of others&quot;). Judging from his past publication history, we can’t afford to immediately dismiss the author as a crank this time.

It seems that a preliminary version of the &lt;a href=&quot;http://www.hpl.hp.com/personal/Vinay_Deolalikar/Papers/pnp_preliminary.pdf&quot;&gt;paper&lt;/a&gt;, dated 6 August (note: there’s a new version &lt;del datetime=&quot;2010-08-10T18:52:54+00:00&quot;&gt;with &quot;minor updates&quot; &lt;a href=&quot;http://www.hpl.hp.com/personal/Vinay_Deolalikar/Papers/pnp_updated.pdf&quot;&gt;here&lt;/a&gt;&lt;/del&gt; &lt;a href=&quot;http://www.hpl.hp.com/personal/Vinay_Deolalikar/Papers/pnp_updated-mjr.pdf&quot;&gt;here&lt;/a&gt;), was circulated by Deolalikar to &quot;several leading researchers in various areas&quot;, rather amusingly in both &quot;10pt and 12pt fonts&quot; (apparently, the proof does not relativise to 11pt). According to Baker, &lt;a href=&quot;http://www.cs.toronto.edu/~sacook/&quot;&gt;Stephen Cook&lt;/a&gt; commented &quot;This appears to be a relatively serious claim to have solved P vs NP&quot;. The story started spreading when someone uploaded the paper to &lt;a href=&quot;http://www.scribd.com/doc/35539144/pnp12pt&quot;&gt;Scribd&lt;/a&gt; without the author’s knowledge.

The paper is very long (&lt;del datetime=&quot;2010-08-09T19:55:39+00:00&quot;&gt;103&lt;/del&gt; &lt;del datetime=&quot;2010-08-10T18:56:17+00:00&quot;&gt;104&lt;/del&gt; 107 pages in the 12pt version) and involves &quot;several fields spanning logic, statistics, graphical models, random ensembles, and statistical physics&quot;. So, before engaging in a serious reading, I’m going to wait for someone more qualified than me to make a preliminary evaluation. In particular, the word &quot;physics&quot; sounds pretty scary to me in this context.

What do computer scientists and related people on the web think about it? Here is a list of people who commented on Deolalikar’s paper (until 10 August).

- Richard Lipton &lt;a href=&quot;http://rjlipton.wordpress.com/2010/08/08/a-proof-that-p-is-not-equal-to-np/&quot;&gt;thinks&lt;/a&gt; this is a &quot;well written paper, by a serious researcher&quot; and &quot;who knows—perhaps this is the solution we have all have been waiting for&quot;.
- Lance Fortnow is &lt;a href=&quot;http://twitter.com/fortnow/status/20673954168&quot;&gt;laconically cautious&lt;/a&gt;: &quot;Much ado about an unverified proof&quot;.
- Dave Bacon &lt;a href=&quot;http://dabacon.org/pontiff/?p=4286&quot;&gt;thinks&lt;/a&gt; &quot;this a fairly serious attack by a serious researcher&quot;.
- Scott Aaronson is willing to supplement the million dollar Clay Millennium prize by $200,000 if the proof turns out to be correct, though he &lt;a href=&quot;http://scottaaronson.com/blog/?p=456&quot;&gt;remarks&lt;/a&gt; that &quot;even if this attempt fails, it seems to introduce some thought-provoking new ideas&quot;.
- Daniel Lemire doesn’t &lt;a href=&quot;http://www.daniel-lemire.com/blog/archives/2010/08/09/how-to-get-everyone-talking-about-your-research/&quot;&gt;comment&lt;/a&gt; directly on the contents the paper, but on some reasons why Deolalikar gets so much attention: well-written paper, and enough reputation to be considered as a peer.
- Suresh Venkatasubramanian seems to &lt;a href=&quot;http://twitter.com/geomblog/status/20713035743&quot;&gt;think&lt;/a&gt; that understanding this paper does indeed require knowledge of many fields. He’s also &lt;a href=&quot;http://geomblog.blogspot.com/2010/08/on-deolalikar-proof-crowdsourcing.html&quot;&gt;proposing&lt;/a&gt; a collaborative effort to identify any issue with the current version of the proof.
- William Gasarch would &lt;a href=&quot;http://blog.computationalcomplexity.org/2010/08/that-p-ne-np-proof-whats-up-with-that.html&quot;&gt;bet&lt;/a&gt; that Deolalikar &quot;proved something very interesting, but not P ≠ NP&quot;: we just haven’t made enough progress on that problem yet.
- Bruce Schneier also &lt;a href=&quot;http://www.schneier.com/blog/archives/2010/08/p_np_1.html&quot;&gt;bets&lt;/a&gt; that the proof is flawed, despite the author being a &quot;respected researcher&quot;.
- Ryan Williams is quite &lt;a href=&quot;http://twitter.com/rrwilliams/status/20741046788&quot;&gt;sceptical&lt;/a&gt; (his technical comments &lt;a href=&quot;http://twitter.com/rrwilliams/status/20741170730&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;http://twitter.com/rrwilliams/status/20741266130&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;http://twitter.com/rrwilliams/status/20741316229&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://twitter.com/rrwilliams/status/20741339603&quot;&gt;here&lt;/a&gt;).
- Richard Lipton and Ken Regan &lt;a href=&quot;http://rjlipton.wordpress.com/2010/08/09/issues-in-the-proof-that-p%E2%89%A0np/&quot;&gt;highlight&lt;/a&gt; some technical issues with the proof.
- In the mean time, Geomblog &lt;a href=&quot;http://geomblog.blogspot.com/2010/08/polymath-home-for-analysis-of.html&quot;&gt;reports&lt;/a&gt; that the analysis of Deolalikar’s proof moved to a &lt;a href=&quot;http://michaelnielsen.org/polymath1/index.php?title=Deolalikar%27s_P%21%3DNP_paper&quot;&gt;new wiki&lt;/a&gt;, in collaboration with the Polymath project.
- Terence Tao &lt;a href=&quot;http://www.google.com/buzz/114134834346472219368/1vfSCPtRQZf/Vinay-Deolaikar-recently-released-a-102-page&quot;&gt;says&lt;/a&gt; &quot;[t]his is a serious effort, but one which sits somewhat orthogonally to the existing work on this problem&quot;, so a sensible evaluation might require quite some time.
- András Salamon is also &quot;&lt;a href=&quot;http://constraints.wordpress.com/2010/08/09/deolalikars-manuscript/&quot;&gt;sceptical&lt;/a&gt;&quot;, as &quot;the technique used is unlikely to work to prove the desired result&quot;. However, &quot;the paper is definitely worth reading&quot; anyway.
- More detailed &lt;a href=&quot;http://twitter.com/rrwilliams/status/20805879147&quot;&gt;comments&lt;/a&gt; by Ryan Williams on the &lt;a href=&quot;http://michaelnielsen.org/polymath1/index.php?title=Deolalikar%27s_P!%3DNP_paper#Issues_with_random_k-SAT&quot;&gt;Polymath wiki&lt;/a&gt;.
- In another &lt;a href=&quot;http://dabacon.org/pontiff/?p=4292&quot;&gt;post&lt;/a&gt;, Dave Bacon comments on the reasons behind the interest drawn by the paper (it’s very broad in scope and contains a lot of review). And, by the way, he and his colleagues are sceptical too.
- Charanjit Jutla &lt;a href=&quot;http://rjlipton.wordpress.com/2010/08/09/issues-in-the-proof-that-p%E2%89%A0np/#comment-4712&quot;&gt;thinks&lt;/a&gt; there’s a &quot;huge (unfixable) gap&quot;.
- As of 10 August, 22:58 UTC there isn’t any mention of the P ≠ NP paper on Deolalikar’s &lt;a href=&quot;http://www.hpl.hp.com/personal/Vinay_Deolalikar/&quot;&gt;home page&lt;/a&gt; anymore (though the paper itself is still available in its &lt;a href=&quot;http://www.hpl.hp.com/personal/Vinay_Deolalikar/Papers/pnp_updated-mjr.pdf&quot;&gt;third version&lt;/a&gt;).

The proof also made it to Gerhard Woeginger’s &lt;a href=&quot;http://www.win.tue.nl/~gwoegi/P-versus-NP.htm&quot;&gt;P-versus-NP page&lt;/a&gt; and, somehow, to &lt;a href=&quot;http://isohunt.com/torrent_details/204525141/?tab=summary&quot;&gt;BitTorrent&lt;/a&gt; (?). &lt;em&gt;Nature news&lt;/em&gt; has an article on this story: &lt;a href=&quot;http://www.nature.com/news/2010/100810/full/news.2010.398.html&quot;&gt;Million-dollar problem cracked?&lt;/a&gt;.

On a side note, I’m genuinely surprised by the amount of &lt;a href=&quot;http://search.twitter.com/search?q=P%20NP&quot;&gt;buzz&lt;/a&gt; on Twitter generated by this story (mostly deriving from &lt;a href=&quot;http://science.slashdot.org/story/10/08/08/226227/Claimed-Proof-That-P--NP&quot;&gt;Slashdot&lt;/a&gt; and &lt;a href=&quot;http://news.ycombinator.net/item?id=1585850&quot;&gt;Hacker News&lt;/a&gt; I think). Independently of the actual correctness of the proof, I believe this is a good thing, as it suggests that there’s enough public interest for the most important open problem in computer science.</content><author><name>Antonio E. Porreca</name><email>porreca@disco.unimib.it</email></author><summary type="html">Note: I’ve stopped updating this post, now that I’ve collected quite a lot of comments. For the latest news, I suggest checking my tweets and, in particular, the Polymath wiki page (edited by more people than just yours truly). Maybe I’ll have another post in the next few days, if something shocking happens.</summary></entry><entry><title type="html">Karl Popper and Turing’s thesis</title><link href="http://localhost:4000/blog/popper-and-turing/" rel="alternate" type="text/html" title="Karl Popper and Turing’s thesis" /><published>2010-08-04T00:00:00+02:00</published><updated>2010-08-04T00:00:00+02:00</updated><id>http://localhost:4000/blog/popper-and-turing</id><content type="html" xml:base="http://localhost:4000/blog/popper-and-turing/">Those of you who have been following me on &lt;a href=&quot;http://twitter.com/aeporreca&quot;&gt;Twitter&lt;/a&gt; may have noticed that, lately, I seem to be tormented by scientifico-philosophical questions related to Turing machines and Turing's thesis.

Indeed I am, because I've chosen this as the subject of an exam paper, a decision that I'm beginning to regret: the matter is so deep and fascinating, and I'm so little qualified to discuss it, that I'm spending an awful amount of time perusing unfamiliar literature without actually writing anything. There's a lifetime to spend just to familiarise with the amazing history of computability, even if we limit ourselves to the first half of the 20th century, and we haven't even begun with the philosophy of science yet!

I decided to directly attack the big ugly question: consider (one particular version of) &lt;a href=&quot;http://plato.stanford.edu/entries/church-turing/&quot;&gt;Turing's thesis&lt;/a&gt;, the claim that every arithmetical function computable by a human being, provided with enough ink and paper, can be also computed by one of &lt;a href=&quot;http://plato.stanford.edu/entries/turing-machine/&quot;&gt;Turing's machines&lt;/a&gt;. Is this a scientific statement?

Deciding how to establish whether something is science or not is called the &lt;a href=&quot;http://en.wikipedia.org/wiki/Demarcation_problem&quot;&gt;demarcation problem&lt;/a&gt;. Having, unfortunately, no time to get a philosophy degree at the moment, I chose a particular point of view, that of &lt;a href=&quot;http://plato.stanford.edu/entries/popper&quot;&gt;Karl Popper&lt;/a&gt; (the philosopher of &lt;em&gt;critical rationalism&lt;/em&gt;), who was one of the main names in the course I attended. Popper says that what makes science science is &lt;em&gt;not&lt;/em&gt; the verifiability of our claims, which is inevitably a hopeless task, but their &lt;em&gt;falsifiability&lt;/em&gt;.

&lt;a href=&quot;http://www.flickr.com/photos/garysutherland/3095267556/&quot; title=&quot;Black Swan by Gary_Sutherland, on Flickr&quot;&gt;&lt;img src=&quot;http://farm4.static.flickr.com/3051/3095267556_91df942a26.jpg&quot; width=&quot;500&quot; height=&quot;375&quot; alt=&quot;Black Swan&quot;&gt;&lt;/a&gt;

Popper rejects induction as a valid inference principle in science, essentially for purely logical reasons. Having seen enough white swans doesn't allow us to infer that all swans must be white, as somewhere else a black one might be hiding and mocking us. However, this must not stop us from making the claim that all swans are white: this is a perfectly reasonable hypothesis, only we should not deceive ourselves and pretend that our claim is somewhat justified by our previous observations.

What makes the white swan hypothesis a good hypothesis is that it can be falsified. Tomorrow, a friend of ours could come to us and produce a black swan: this falsifies our claim, and we should (in principle) leave that one behind, and make science advance by formulating another.

Popper says: that's what science is made of! Formulate &lt;a href=&quot;http://books.google.com/books?id=Yq6xeupNStMC&amp;lpg=PA280&amp;dq=popper%20%22bold%20ideas%22&amp;pg=PA280#v=onepage&amp;q=%22bold%20ideas%22&amp;f=false&quot;&gt;&lt;em&gt;bold&lt;/em&gt;&lt;/a&gt; statements, using whatever means you find convenient: observations, deduction from metaphysical principles, imagination; it doesn't really matter. And the bolder you are, the better: a more falsifiable statement is more informative, as it excludes a lot of possible worlds. Then you must fiercely attack your theory, by making lots and lots of empirical observations, with the goal not of proving it, but of &lt;em&gt;making it fail&lt;/em&gt;.

And the testing never ends. It's only a matter of convention to stop trying to falsify our theory for a moment, and do something else. Because we can always resume our experiments later, when doubts start troubling us again. In his &lt;a href=&quot;http://books.google.com/books?id=Yq6xeupNStMC&amp;pg=PA32&amp;dq=%22The+game+of+science+is,+in+principle,+without+end%22&amp;hl=en&amp;ei=PaJZTO3eFseoOKXbyaMJ&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=9&amp;ved=0CE8Q6AEwCA#v=onepage&amp;q=%22The%20game%20of%20science%20is%2C%20in%20principle%2C%20without%20end%22&amp;f=false&quot;&gt;&lt;em&gt;Logik der Forschung&lt;/em&gt;&lt;/a&gt; (translated as &lt;em&gt;The Logic of Scientific Discovery&lt;/em&gt;) Popper makes his point in a striking way.

&lt;blockquote style=&quot;border:none;&quot;&gt;
The game of science is, in principle, without end. He who decides one day that scientific statements do not call for any further test, and that they can be regarded as finally verified, retires from the game.
&lt;/blockquote&gt;

Subscribing to Popper's view, I'll make my own bold (philosophical) claim, i.e., that Turing's thesis &lt;em&gt;is&lt;/em&gt; a scientific statement. Details in the next episodes.

In the mean time, here is a nice short introduction to Popper's philosophy of science: &lt;a href=&quot;http://www.jstor.org/stable/3748471&quot;&gt;Popper's account of scientific method&lt;/a&gt; by J. A. Passmore.</content><author><name>Antonio E. Porreca</name><email>porreca@disco.unimib.it</email></author><summary type="html">Those of you who have been following me on Twitter may have noticed that, lately, I seem to be tormented by scientifico-philosophical questions related to Turing machines and Turing’s thesis.</summary></entry></feed>